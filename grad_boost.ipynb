{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Градиентный бустинг (gradient boosting)** — это метод машинного обучения, который позволяет последовательно улучшать качество предсказаний, комбинируя слабые модели (например, решающие деревья) в сильную модель.\n",
        "\n",
        "Основная идея заключается в том, чтобы обучить новую модель, которая будет исправлять \"ошибки\" предыдущей модели, постепенно улучшая качество предсказаний на каждой итерации.\n",
        "\n",
        "Таким образом, градиентный бустинг является итеративным методом, который обучает модели последовательно. В результате ансамбль моделей становится все более точным и дает лучшие предсказания, чем отдельно взятая слабая модель."
      ],
      "metadata": {
        "id": "FoVeYcxfwpb9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Первый базовый алгоритм**\n"
      ],
      "metadata": {
        "id": "jZ8PioYp06Rp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "class GradientBoostingRegressor:\n",
        "    \"\"\"Gradient boosting regressor.\"\"\"\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Fit the model to the data.\n",
        "\n",
        "        Args:\n",
        "            X: array-like of shape (n_samples - x.shape[0], n_features - x.shape[1])\n",
        "            y: array-like of shape (n_samples,)\n",
        "\n",
        "        Returns:\n",
        "            GradientBoostingRegressor: The fitted model.\n",
        "        \"\"\"\n",
        "        #среднее значение по выборке. Сохраните среднее значение в дополнительный атрибут класса self.base_pred_\n",
        "        self.base_pred_ = np.mean(y)\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Predict the target of new data.\n",
        "\n",
        "        Args:\n",
        "            X: array-like of shape (n_samples, n_features)\n",
        "\n",
        "        Returns:\n",
        "            y: array-like of shape (n_samples,)\n",
        "            The predict values.\n",
        "\n",
        "        \"\"\"\n",
        "        # делал прогноз для всех значений средним значением по обучающей выборке.\n",
        "        if hasattr(X, \"__len__\"):\n",
        "            n = len(X)\n",
        "        else:\n",
        "            n = X.shape[0]\n",
        "\n",
        "        predictions = np.full(n,self.base_pred_)\n",
        "\n",
        "        return predictions"
      ],
      "metadata": {
        "id": "gzET5m7d1m2M"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.random.randn(5, 3)\n",
        "y = np.array([10, 12, 13, 14, 15])\n",
        "\n",
        "model = GradientBoostingRegressor()\n",
        "model.fit(X, y)\n",
        "\n",
        "print(model.base_pred_)  # Должно быть 12.8\n",
        "print(model.predict(X))  # Массив из 12.8 длины 5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y9kHiWcP3oES",
        "outputId": "dc879782-33f3-4ab0-dafd-136d1b667fbb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12.8\n",
            "[12.8 12.8 12.8 12.8 12.8]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Мы разобрали, что каждое новое дерево в ансамбле обучается на суммарной ошибке предыдущих деревьев.**"
      ],
      "metadata": {
        "id": "gD3ox-FHY9sP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Поэтому любую константу в векторе градиента можно сократить, потому что она компенсируется другим значением шага кратным этой константе."
      ],
      "metadata": {
        "id": "a2D0ZTpAdkYr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from typing import Tuple\n",
        "\n",
        "\n",
        "def mse(y_true: np.ndarray, y_pred: np.ndarray) -> Tuple[float, np.ndarray]:\n",
        "    \"\"\"Mean squared error loss function and gradient.\"\"\"\n",
        "    # YOUR CODE HERE\n",
        "    # loss - значение функции потерь (усредненное по всем объектам)\n",
        "    # grad - вектор псевдо-остатков для каждого объекта\n",
        "    loss = np.mean(y_pred - y_true) ** 2\n",
        "    grad = y_pred - y_true\n",
        "    return loss, grad\n",
        "\n",
        "def mae(y_true: np.ndarray, y_pred: np.ndarray) -> Tuple[float, np.ndarray]:\n",
        "    \"\"\"Mean absolute error loss function and gradient.\"\"\"\n",
        "    # YOUR CODE HERE\n",
        "    loss = np.mean(np.abs(y_pred - y_true))\n",
        "    grad = np.sign(y_pred - y_true)\n",
        "    return loss, grad"
      ],
      "metadata": {
        "id": "jpsQbt7WVst_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Доработать GradBoost**"
      ],
      "metadata": {
        "id": "oNBaZP1biDNm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "class GradientBoostingRegressor:\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_estimators=100,\n",
        "        learning_rate=0.1,\n",
        "        max_depth=3,\n",
        "        min_samples_split=2,\n",
        "        loss=\"mse\",\n",
        "        verbose=False,\n",
        "    ):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.loss = loss\n",
        "        self.verbose = verbose\n",
        "        self.base_pred_ = None\n",
        "        self.trees_ = []\n",
        "\n",
        "    def _mse(self, y_true, y_pred):\n",
        "        # loss (усредненная), градиент (остатки)\n",
        "        loss = np.mean((y_true - y_pred) ** 2)\n",
        "        grad = y_pred - y_true\n",
        "        return loss, grad\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.base_pred_ = np.mean(y)\n",
        "        y_pred = np.full_like(y, self.base_pred_, dtype=float)\n",
        "        self.trees_ = []\n",
        "\n",
        "        # Определяем функцию потерь и градиент\n",
        "        if callable(self.loss):\n",
        "            loss_func = self.loss\n",
        "        elif self.loss == \"mse\":\n",
        "            loss_func = self._mse\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown loss: {self.loss}\")\n",
        "        for i in range(self.n_estimators):\n",
        "            # 1. Считаем градиент (антиградиент)\n",
        "            loss, grad = loss_func(y, y_pred)\n",
        "            anti_grad = -grad\n",
        "            # 2. Обучаем дерево на антиградиентах\n",
        "            tree = DecisionTreeRegressor(max_depth=self.max_depth,\n",
        "                                         min_samples_split=self.min_samples_split)\n",
        "            tree.fit(X, anti_grad)\n",
        "            # 3. Вычисляем предсказания дерева, делаем шаг градиентного спуска\n",
        "            tree_pred = tree.predict(X)\n",
        "            y_pred += self.learning_rate * tree_pred\n",
        "            # 4. Сохраняем дерево\n",
        "            self.trees_.append(tree)\n",
        "            # 5. Если надо, выводим loss\n",
        "            if self.verbose:\n",
        "                current_loss, _ = loss_func(y, y_pred)\n",
        "                print(f\"Step {i+1}/{self.n_estimators}, Loss: {current_loss:.5f}\")\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        # 1. Предсказание средним\n",
        "        n = X.shape[0]\n",
        "        y_pred = np.full(n, self.base_pred_, dtype=float)\n",
        "        for tree in self.trees_:\n",
        "            y_pred += self.learning_rate * tree.predict(X)\n",
        "        return y_pred"
      ],
      "metadata": {
        "id": "aPmfzw0aiGVF"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cтохастический градиентный спуск (SGD — stochastic gradient descent). Стохастический — т.е. случайный.**\n",
        "\n",
        "Использование SGD помогает внести случайность и разнообразие в построение каждого дерева, что может снизить переобучение и улучшить обобщающую способность модели."
      ],
      "metadata": {
        "id": "gZ-DJpOyzcFk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "class GradientBoostingRegressor:\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_estimators=100,\n",
        "        learning_rate=0.1,\n",
        "        max_depth=3,\n",
        "        min_samples_split=2,\n",
        "        loss=\"mse\",\n",
        "        verbose=False,\n",
        "        subsample_size=0.5,\n",
        "        replace=False,\n",
        "    ):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.loss = loss\n",
        "        self.verbose = verbose\n",
        "        self.subsample_size = subsample_size\n",
        "        self.replace = replace\n",
        "        self.base_pred_ = None\n",
        "        self.trees_ = []\n",
        "\n",
        "    def _mse(self, y_true, y_pred):\n",
        "        loss = np.mean((y_true - y_pred) ** 2)\n",
        "        grad = y_pred - y_true\n",
        "        return loss, grad\n",
        "\n",
        "    def _subsample(self, X, y):\n",
        "        n_samples = X.shape[0]\n",
        "        sample_size = int(self.subsample_size * n_samples)\n",
        "        idx = np.random.choice(n_samples, size=sample_size, replace=self.replace)\n",
        "        return X[idx], y[idx]\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.base_pred_ = np.mean(y)\n",
        "        y_pred = np.full_like(y, self.base_pred_, dtype=float)\n",
        "        self.trees_ = []\n",
        "\n",
        "        if callable(self.loss):\n",
        "            loss_func = self.loss\n",
        "        elif self.loss == \"mse\":\n",
        "            loss_func = self._mse\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown loss: {self.loss}\")\n",
        "\n",
        "        for i in range(self.n_estimators):\n",
        "            loss, grad = loss_func(y, y_pred)\n",
        "            anti_grad = -grad\n",
        "\n",
        "            sub_X, sub_anti_grad = self._subsample(X, anti_grad)\n",
        "\n",
        "            tree = DecisionTreeRegressor(\n",
        "                max_depth=self.max_depth,\n",
        "                min_samples_split=self.min_samples_split\n",
        "            )\n",
        "            tree.fit(sub_X, sub_anti_grad)\n",
        "\n",
        "            tree_pred = tree.predict(X)\n",
        "            y_pred += self.learning_rate * tree_pred\n",
        "            self.trees_.append(tree)\n",
        "\n",
        "            if self.verbose:\n",
        "                current_loss, _ = loss_func(y, y_pred)\n",
        "                print(f\"Step {i+1}/{self.n_estimators}, Loss: {current_loss:.5f}\")\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        n = X.shape[0]\n",
        "        y_pred = np.full(n, self.base_pred_, dtype=float)\n",
        "        for tree in self.trees_:\n",
        "            y_pred += self.learning_rate * tree.predict(X)\n",
        "        return y_pred"
      ],
      "metadata": {
        "id": "pUqAdkzVzfOi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}