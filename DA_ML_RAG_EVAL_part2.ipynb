{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **RAG-система**"
      ],
      "metadata": {
        "id": "wMTDB5OEmjvZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Основные этапы:\n",
        "1. **Энкодер и векториация**\n",
        "2. **Получение эмбеддингов:**\n",
        "Документы из базы знаний заранее преобразуются в эмбеддинги с помощью выбранного энкодера, а запрос преобразовывается при поступлении.\n",
        "3. **Анализ сходства между запросом и документами:**\n",
        "Вычисляется косинусное сходство между эмбеддингом запроса и эмбеддингами документов. Косинусное сходство измеряет угол между двумя векторами в пространстве и варьируется от -1 до 1, где 1 означает полное совпадение, а 0 – отсутствие сходства. Таким образом документы с наибольшим косинусным сходством по отношению к запросу считаются наиболее релевантными и извлекаются для дальнейшего использования."
      ],
      "metadata": {
        "id": "-60FXulPmxaY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Tuple, Union\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import torch\n",
        "\n",
        "class Encoder:\n",
        "    \"\"\"\n",
        "    Encoder class for generating embeddings from textual data using a SentenceTransformer model.\n",
        "    \"\"\"\n",
        "    def __init__(self, model_name: str = 'cointegrated/rubert-tiny2', use_gpu: bool = False):\n",
        "        \"\"\"\n",
        "        Initializes the Encoder with the given model name and device configuration.\n",
        "        \"\"\"\n",
        "        if not model_name:\n",
        "            raise ValueError(\"Model name cannot be empty.\")\n",
        "\n",
        "        try:\n",
        "            self.device = 'cuda' if (use_gpu and torch.cuda.is_available()) else 'cpu'\n",
        "            self.model = SentenceTransformer(model_name, device=self.device)\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Failed to load model '{model_name}': {str(e)}\")\n",
        "\n",
        "    def encode(self, data: Union[List[str], str]) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Encodes text(s) into embeddings.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            if isinstance(data, str):\n",
        "                data = [data]\n",
        "            embeddings = self.model.encode(data, convert_to_tensor=True, device=self.device)\n",
        "            return embeddings\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Encoding failed: {str(e)}\")\n",
        "\n",
        "class RAG:\n",
        "    \"\"\"\n",
        "    Retrieval-Augmented Generation (RAG) class.\n",
        "    \"\"\"\n",
        "    def __init__(self, encoder: Encoder):\n",
        "        \"\"\"\n",
        "        Initializes the RAG class with the given encoder.\n",
        "        \"\"\"\n",
        "        if not isinstance(encoder, Encoder):\n",
        "            raise ValueError(\"The encoder must be an instance of Encoder.\")\n",
        "        self.encoder = encoder\n",
        "        self.documents = []\n",
        "        self.doc_embeddings = None\n",
        "\n",
        "    def fit(self, documents: List[str]):\n",
        "        \"\"\"\n",
        "        Encodes and stores document embeddings.\n",
        "        \"\"\"\n",
        "        if not documents:\n",
        "            raise ValueError(\"Document list cannot be empty.\")\n",
        "        self.documents = documents\n",
        "        try:\n",
        "            self.doc_embeddings = self.encoder.encode(documents)\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Document encoding failed: {str(e)}\")\n",
        "\n",
        "    def retrieve(self, query: str, retrieval_limit: int = 5, similarity_threshold: float = 0.5) -> Tuple[List[int], List[str]]:\n",
        "        \"\"\"\n",
        "        Retrieves top-k most relevant documents for the query.\n",
        "        \"\"\"\n",
        "        if self.doc_embeddings is None:\n",
        "            raise ValueError(\"You must call fit() before retrieve().\")\n",
        "        if not (1 <= retrieval_limit <= 10):\n",
        "            raise ValueError(\"retrieval_limit must be between 1 and 10.\")\n",
        "        if retrieval_limit > len(self.documents):\n",
        "            raise ValueError(\"retrieval_limit cannot exceed number of documents.\")\n",
        "        if not (0.0 <= similarity_threshold <= 1.0):\n",
        "            raise ValueError(\"similarity_threshold must be between 0 and 1.\")\n",
        "\n",
        "        try:\n",
        "            #Кодирование запроса в вектор\n",
        "            query_embedding = self.encoder.encode(query)\n",
        "            #Расчёт косинусной схожести - как logit\n",
        "            cosine_scores = util.cos_sim(query_embedding, self.doc_embeddings)[0]  # shape: (num_docs)\n",
        "            #Выбор топ-N документов\n",
        "            top_results = torch.topk(cosine_scores, k=retrieval_limit)\n",
        "\n",
        "            relevant_indices = top_results.indices.tolist()\n",
        "            relevant_scores = top_results.values.tolist()\n",
        "\n",
        "            filtered_indices = [\n",
        "                idx for idx, score in zip(relevant_indices, relevant_scores)\n",
        "                if score >= similarity_threshold\n",
        "            ]\n",
        "\n",
        "            retrieved_docs = [self.documents[idx] for idx in filtered_indices]\n",
        "            return filtered_indices, retrieved_docs\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Retrieval failed: {str(e)}\")\n",
        "\n",
        "    def _create_prompt_template(self, query: str, retrieved_docs: List[str]) -> str:\n",
        "        \"\"\"\n",
        "        Creates a prompt template for generation.\n",
        "        \"\"\"\n",
        "\n",
        "        prompt = \"Instructions: Based on the relevant documents, generate a comprehensive response to the user's query.\\n\\n\"\n",
        "        prompt += \"Relevant Documents:\\n\"\n",
        "        for i, doc in enumerate(retrieved_docs):\n",
        "            prompt += f\"Document {i+1}: {doc}\\n\"\n",
        "        prompt += f\"\\nUser Query: {query}\\n\"\n",
        "        prompt += \"Answer:\"\n",
        "        return prompt\n",
        "\n",
        "    def _generate(self, query: str, retrieved_docs: List[str]) -> str:\n",
        "        \"\"\"\n",
        "        Placeholder for text generation logic.\n",
        "        \"\"\"\n",
        "        prompt = self._create_prompt_template(query, retrieved_docs)\n",
        "\n",
        "        generated_response = f\"(Simulated Response based on documents and query: '{query}')\"\n",
        "        return generated_response\n",
        "\n",
        "    def run(self, query: str) -> str:\n",
        "        \"\"\"\n",
        "        Runs full RAG pipeline.\n",
        "        \"\"\"\n",
        "        _, retrieved_docs = self.retrieve(query)\n",
        "        return self._generate(query, retrieved_docs)"
      ],
      "metadata": {
        "id": "brre-14gYVe7"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents = [\n",
        "    \"Machine learning is a method of data analysis that automates analytical model building.\",\n",
        "    \"Artificial intelligence is intelligence demonstrated by machines, in contrast to the natural intelligence displayed by humans.\",\n",
        "    \"Natural language processing is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language.\",\n",
        "    \"Deep learning is a class of machine learning algorithms that uses multiple layers to progressively extract higher-level features from the raw input.\"\n",
        "]\n",
        "\n",
        "encoder = Encoder()\n",
        "rag = RAG(encoder)\n",
        "rag.fit(documents)\n",
        "\n",
        "query = \"Tell me about deep learning.\"\n",
        "result_indices, result_documents  = rag.retrieve(query, retrieval_limit=2, similarity_threshold=0.6)\n",
        "\n",
        "print(f'Result indices: {result_indices}')\n",
        "print(f'Result documents: {result_documents}')\n",
        "\n",
        "# >> Output:\n",
        "# >> Result indices: [3, 0]\n",
        "# >> Result documents:\n",
        "# >> >> 'Deep learning is a class of machine learning algorithms that uses multiple layers to progressively extract higher-level features from the raw input.',\n",
        "# >> >> 'Machine learning is a method of data analysis that automates analytical model building.'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bzY1LlXplzrU",
        "outputId": "456cb271-d5db-4189-b331-f7baaeebd19b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result indices: [3, 0]\n",
            "Result documents: ['Deep learning is a class of machine learning algorithms that uses multiple layers to progressively extract higher-level features from the raw input.', 'Machine learning is a method of data analysis that automates analytical model building.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Анализ базы знаний**"
      ],
      "metadata": {
        "id": "uUCeZly0mabz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RAG Retrieval Score**"
      ],
      "metadata": {
        "id": "nje3MHg2ohHn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Tuple\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import torch\n",
        "import json\n",
        "import os\n",
        "\n",
        "\n",
        "class Encoder:\n",
        "    def __init__(self, model_name: str = 'cointegrated/rubert-tiny2', use_gpu: bool = False):\n",
        "        if not model_name:\n",
        "            raise ValueError(\"Model name cannot be empty.\")\n",
        "\n",
        "        try:\n",
        "            self.device = 'cuda' if (use_gpu and torch.cuda.is_available()) else 'cpu'\n",
        "            self.model = SentenceTransformer(model_name, device=self.device)\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Failed to load model '{model_name}': {str(e)}\")\n",
        "\n",
        "    def encode(self, data):\n",
        "        try:\n",
        "            if isinstance(data, str):\n",
        "                data = [data]\n",
        "            embeddings = self.model.encode(data, convert_to_tensor=True, device=self.device)\n",
        "            return embeddings\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Encoding failed: {str(e)}\")\n",
        "\n",
        "\n",
        "class RAG:\n",
        "    def __init__(self, encoder: Encoder):\n",
        "        if not isinstance(encoder, Encoder):\n",
        "            raise ValueError(\"The encoder must be an instance of Encoder.\")\n",
        "        self.encoder = encoder\n",
        "        self.documents = []\n",
        "        self.doc_embeddings = None\n",
        "\n",
        "    def fit(self, documents: List[str]):\n",
        "        if not documents:\n",
        "            raise ValueError(\"Document list cannot be empty.\")\n",
        "        self.documents = documents\n",
        "        try:\n",
        "            self.doc_embeddings = self.encoder.encode(documents)\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Document encoding failed: {str(e)}\")\n",
        "\n",
        "    def retrieve(self, query: str, retrieval_limit: int = 5, similarity_threshold: float = 0.5) -> Tuple[List[int], List[str]]:\n",
        "        if self.doc_embeddings is None:\n",
        "            raise ValueError(\"You must call fit() before retrieve().\")\n",
        "        if not (1 <= retrieval_limit <= 10):\n",
        "            raise ValueError(\"retrieval_limit must be between 1 and 10.\")\n",
        "        if retrieval_limit > len(self.documents):\n",
        "            raise ValueError(\"retrieval_limit cannot exceed number of documents.\")\n",
        "        if not (0.0 <= similarity_threshold <= 1.0):\n",
        "            raise ValueError(\"similarity_threshold must be between 0 and 1.\")\n",
        "\n",
        "        try:\n",
        "            query_embedding = self.encoder.encode(query)\n",
        "            cosine_scores = util.cos_sim(query_embedding, self.doc_embeddings)[0]\n",
        "            top_results = torch.topk(cosine_scores, k=retrieval_limit)\n",
        "\n",
        "            relevant_indices = top_results.indices.tolist()\n",
        "            relevant_scores = top_results.values.tolist()\n",
        "\n",
        "            filtered_indices = [\n",
        "                idx for idx, score in zip(relevant_indices, relevant_scores) if score >= similarity_threshold\n",
        "            ]\n",
        "            retrieved_docs = [self.documents[idx] for idx in filtered_indices]\n",
        "            return filtered_indices, retrieved_docs\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Retrieval failed: {str(e)}\")\n",
        "\n",
        "\n",
        "class RAGEval:\n",
        "    def __init__(\n",
        "        self,\n",
        "        documents_path: str,\n",
        "        questions_path: str,\n",
        "        retrieval_limit: int = 5,\n",
        "        similarity_threshold: float = 0.5\n",
        "    ):\n",
        "        self.documents = self.load_documents(documents_path)\n",
        "        self.questions = self.load_questions(questions_path)\n",
        "        self.retrieval_limit = retrieval_limit\n",
        "        self.similarity_threshold = similarity_threshold\n",
        "\n",
        "        if not self.documents:\n",
        "            raise ValueError(\"The documents list is empty.\")\n",
        "        if not self.questions:\n",
        "            raise ValueError(\"The questions list is empty.\")\n",
        "\n",
        "        self.encoder = Encoder()\n",
        "        self.rag = RAG(self.encoder)\n",
        "        self.rag.fit(self.documents)\n",
        "\n",
        "    def load_documents(self, path: str) -> List[str]:\n",
        "        if not os.path.exists(path):\n",
        "            raise FileNotFoundError(f\"File {path} does not exist\")\n",
        "        try:\n",
        "            with open(path, encoding=\"utf-8\") as f:\n",
        "                data = json.load(f)\n",
        "        except json.JSONDecodeError:\n",
        "            raise ValueError(\"Invalid JSON file for documents\")\n",
        "\n",
        "        if not isinstance(data, list):\n",
        "            raise ValueError(\"Documents JSON must contain a list\")\n",
        "\n",
        "        return [self.validate_document(doc) for doc in data]\n",
        "\n",
        "    def validate_document(self, doc) -> str:\n",
        "        if not isinstance(doc, dict):\n",
        "            raise ValueError(\"Each document must be a dictionary.\")\n",
        "        if 'content' not in doc:\n",
        "            raise ValueError(\"Each document must contain a 'content' field.\")\n",
        "        return doc['content']\n",
        "\n",
        "    def load_questions(self, path: str) -> List[str]:\n",
        "        if not os.path.exists(path):\n",
        "            raise FileNotFoundError(f\"File {path} does not exist\")\n",
        "        try:\n",
        "            with open(path, encoding=\"utf-8\") as f:\n",
        "                data = json.load(f)\n",
        "        except json.JSONDecodeError:\n",
        "            raise ValueError(\"Invalid JSON file for questions\")\n",
        "\n",
        "        if not isinstance(data, list):\n",
        "            raise ValueError(\"Questions JSON must contain a list\")\n",
        "\n",
        "        return [self.validate_question(q) for q in data]\n",
        "\n",
        "    def validate_question(self, question) -> str:\n",
        "        if not isinstance(question, dict):\n",
        "            raise ValueError(\"Each question must be a dictionary.\")\n",
        "        if 'question' not in question:\n",
        "            raise ValueError(\"Each question must contain a 'question' field.\")\n",
        "        return question['question']\n",
        "\n",
        "    def evaluate(self, threshold: int = 1) -> Tuple[float, List[int], List[int]]:\n",
        "        doc_hit_count = [0] * len(self.documents)\n",
        "        questions_wo_docs = []\n",
        "\n",
        "        for q_idx, question in enumerate(self.questions):\n",
        "            retrieved_indices, _ = self.rag.retrieve(\n",
        "                query=question,\n",
        "                retrieval_limit=self.retrieval_limit,\n",
        "                similarity_threshold=self.similarity_threshold\n",
        "            )\n",
        "\n",
        "            if not retrieved_indices:\n",
        "                questions_wo_docs.append(q_idx)\n",
        "            else:\n",
        "                for idx in retrieved_indices:\n",
        "                    doc_hit_count[idx] += 1\n",
        "\n",
        "        # Бесполезные документы\n",
        "        useless_docs = [i for i, count in enumerate(doc_hit_count) if count < threshold]\n",
        "\n",
        "        # Метрика RAG Retrieval Score\n",
        "        portion_useless_docs = len(useless_docs) / len(self.documents)\n",
        "        portion_unanswered_questions = len(questions_wo_docs) / len(self.questions)\n",
        "\n",
        "        rag_score = 1 - portion_useless_docs - portion_unanswered_questions\n",
        "\n",
        "        return rag_score, useless_docs, questions_wo_docs"
      ],
      "metadata": {
        "id": "jdmET9bjqIhU"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval = RAGEval('documents.json', 'questions.json', retrieval_limit=5, similarity_threshold=0.6)\n",
        "rag_score, useless_docs, questions_wo_docs = eval.evaluate()\n",
        "\n",
        "print(f'RAG Retrieval Score: {rag_score:0.2f}')\n",
        "print(f'Useless documents [{len(useless_docs)}]: {useless_docs}')\n",
        "print(f'Questions without relevant documents [{len(questions_wo_docs)}]: {questions_wo_docs}')\n",
        "\n",
        "# >> Output:\n",
        "# >> RAG Retrieval Score: 0.55\n",
        "# >> Useless documents [19]: [20, 21, 37, 41, 44, 50, 61, 65, 68, 74, 85, 89, 92, 98, 102, 109, 113, 122, 126]\n",
        "# >> Questions without relevant documents [48]: [98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 124, 125, 127, 129, 130, 131, 132, 133, 135, 137, 139, 140, 141, 142, 143, 146, 149, 151, 152, 153, 154, 155, 156, 157]"
      ],
      "metadata": {
        "id": "jgpo-yu8URO6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a31a7d16-60e6-461e-dd6c-efaa299338a0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RAG Retrieval Score: 0.92\n",
            "Useless documents [3]: [37, 84, 95]\n",
            "Questions without relevant documents [8]: [114, 119, 122, 124, 127, 139, 146, 156]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**QCluster**"
      ],
      "metadata": {
        "id": "dqhI558RotUr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Dict, Union\n",
        "from sklearn.cluster import KMeans\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import torch\n",
        "\n",
        "class Encoder:\n",
        "    def __init__(self, model_name: str = 'cointegrated/rubert-tiny2', use_gpu: bool = False):\n",
        "        if not model_name:\n",
        "            raise ValueError(\"Model name cannot be empty.\")\n",
        "\n",
        "        try:\n",
        "            self.device = 'cuda' if (use_gpu and torch.cuda.is_available()) else 'cpu'\n",
        "            self.model = SentenceTransformer(model_name, device=self.device)\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Failed to load model '{model_name}': {str(e)}\")\n",
        "\n",
        "    def encode(self, data):\n",
        "        try:\n",
        "            if isinstance(data, str):\n",
        "                data = [data]\n",
        "            embeddings = self.model.encode(data, convert_to_tensor=True, device=self.device)\n",
        "            return embeddings\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Encoding failed: {str(e)}\")\n",
        "\n",
        "\n",
        "class QCluster:\n",
        "    \"\"\"Класс для кластеризации вопросов с использованием k-means.\"\"\"\n",
        "\n",
        "    def __init__(self, questions_idx: List[int], questions: List[str]):\n",
        "        \"\"\"\n",
        "        Инициализирует QCluster с индексами вопросов и самими вопросами.\n",
        "\n",
        "        Args:\n",
        "            questions_idx: Список индексов вопросов\n",
        "            questions: Список текстов вопросов\n",
        "\n",
        "        Raises:\n",
        "            ValueError: Если списки разной длины или пустые\n",
        "        \"\"\"\n",
        "        if len(questions_idx) != len(questions):\n",
        "            raise ValueError(\"Списки индексов и вопросов должны быть одинаковой длины\")\n",
        "        if not questions_idx or not questions:\n",
        "            raise ValueError(\"Списки вопросов не могут быть пустыми\")\n",
        "\n",
        "        self.questions_idx = questions_idx\n",
        "        self.questions = questions\n",
        "        self.encoder = Encoder()\n",
        "        self.clusters = {}\n",
        "\n",
        "    def cluster(self, n_clusters: int, show_results: bool = False) -> Dict[int, List[int]]:\n",
        "        \"\"\"\n",
        "        Кластеризует вопросы с помощью k-means.\n",
        "\n",
        "        Args:\n",
        "            n_clusters: Количество кластеров (от 1 до 10)\n",
        "            show_results: Показывать ли результаты (по умолчанию False)\n",
        "\n",
        "        Returns:\n",
        "            Словарь {метка_кластера: [индексы_вопросов]}\n",
        "\n",
        "        Raises:\n",
        "            ValueError: Если n_clusters не в диапазоне 1-10\n",
        "        \"\"\"\n",
        "        if not 1 <= n_clusters <= 10:\n",
        "            raise ValueError(\"Количество кластеров должно быть от 1 до 10\")\n",
        "\n",
        "        # Преобразуем вопросы в векторные представления\n",
        "        embeddings = self.encoder.encode(self.questions).cpu().numpy()\n",
        "\n",
        "        # Кластеризация K-means\n",
        "        kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "        labels = kmeans.fit_predict(embeddings)\n",
        "\n",
        "        # Формируем словарь кластеров\n",
        "        self.clusters = {}\n",
        "        for idx, label in enumerate(labels):\n",
        "          label = int(label)  # гарантируем тип int\n",
        "          if label not in self.clusters:\n",
        "            self.clusters[label] = []\n",
        "          self.clusters[label].append(self.questions_idx[idx])\n",
        "\n",
        "        # Вывод результатов при необходимости\n",
        "        if show_results:\n",
        "            self.print_clusters()\n",
        "\n",
        "        return self.clusters\n",
        "\n",
        "    def print_clusters(self):\n",
        "        \"\"\"Выводит кластеры с вопросами и их индексами.\"\"\"\n",
        "        if not self.clusters:\n",
        "            print(\"Нет кластеров для отображения. Сначала выполните cluster().\")\n",
        "            return\n",
        "\n",
        "        # Сортируем кластеры по метке\n",
        "        for cluster_id in sorted(self.clusters.keys()):\n",
        "            print(f\"\\nКластер {cluster_id}:\")\n",
        "\n",
        "            # Выводим вопросы для каждого кластера\n",
        "            for q_idx in self.clusters[cluster_id]:\n",
        "                try:\n",
        "                    # Находим текст вопроса по оригинальному индексу\n",
        "                    idx_in_list = self.questions_idx.index(q_idx)\n",
        "                    question_text = self.questions[idx_in_list]\n",
        "                    print(f\"- {question_text} (Индекс: {q_idx})\")\n",
        "                except ValueError:\n",
        "                    print(f\"- [!] Вопрос с индексом {q_idx} не найден\")\n",
        "\n",
        "eval = RAGEval('documents.json', 'questions.json', retrieval_limit=5, similarity_threshold=0.6)\n",
        "rag_score, useless_docs_idx, questions_wo_docs_idx = eval.evaluate()\n",
        "questions_wo_docs = [eval.questions[i] for i in questions_wo_docs_idx]\n",
        "\n",
        "qcluster = QCluster(questions_wo_docs_idx, questions_wo_docs)\n",
        "clusters = qcluster.cluster(n_clusters=4, show_results=True)\n",
        "\n",
        "print(f'Raw clusters: {clusters}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-hI3gv-BGAB",
        "outputId": "4f326f54-462c-499e-a06d-9bf1f7443acf"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Кластер 0:\n",
            "- Как объединить ветки в GitHub? (Индекс: 119)\n",
            "- Как создать релиз на GitHub? (Индекс: 122)\n",
            "- Как разрешить конфликты при слиянии веток на GitHub? (Индекс: 127)\n",
            "- Как развернуть Telegram-бота? (Индекс: 156)\n",
            "\n",
            "Кластер 1:\n",
            "- Производительность запросов в PostgreSQL? (Индекс: 146)\n",
            "\n",
            "Кластер 2:\n",
            "- Что такое GitHub и для чего он используется? (Индекс: 114)\n",
            "- Что такое GitHub Pages и как их использовать? (Индекс: 124)\n",
            "\n",
            "Кластер 3:\n",
            "- Скользящее среднее PostgreSQL (Индекс: 139)\n",
            "Raw clusters: {2: [114, 124], 0: [119, 122, 127, 156], 3: [139], 1: [146]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Обновление базы знаний**"
      ],
      "metadata": {
        "id": "Zf04N84XIasP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Загрузка базы знаний\n",
        "with open('documents.json', 'r', encoding='utf-8') as f:\n",
        "    documents = json.load(f)\n",
        "\n",
        "# Удалим документы с индексами useless_docs_idx\n",
        "filtered_documents = [doc for idx, doc in enumerate(documents) if idx not in useless_docs_idx]\n",
        "\n",
        "# Пересохраним очищенный documents.json\n",
        "with open('documents.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(filtered_documents, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f'Удалено {len(documents) - len(filtered_documents)} неиспользуемых документов.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cvLkXnd2HcTQ",
        "outputId": "3e86469c-8e83-4de7-86f4-f939d3b544a8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Удалено 3 неиспользуемых документов.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clusters = qcluster.clusters\n",
        "cluster_texts = []\n",
        "\n",
        "for cluster_id, question_ids in clusters.items():\n",
        "    questions_texts = [\n",
        "        qcluster.questions[qcluster.questions_idx.index(qid)]\n",
        "        for qid in question_ids\n",
        "    ]\n",
        "\n",
        "    # Запоминаем все вопросы кластера\n",
        "    cluster_texts.append((cluster_id, questions_texts))"
      ],
      "metadata": {
        "id": "0YXyR6YfI_cu"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_texts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jnLByj7CJSBR",
        "outputId": "5a62abc7-cd5a-4c36-f731-3b0b8eb9ac22"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(2,\n",
              "  ['Что такое GitHub и для чего он используется?',\n",
              "   'Что такое GitHub Pages и как их использовать?']),\n",
              " (0,\n",
              "  ['Как объединить ветки в GitHub?',\n",
              "   'Как создать релиз на GitHub?',\n",
              "   'Как разрешить конфликты при слиянии веток на GitHub?',\n",
              "   'Как развернуть Telegram-бота?']),\n",
              " (3, ['Скользящее среднее PostgreSQL']),\n",
              " (1, ['Производительность запросов в PostgreSQL?'])]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('documents.json', 'r', encoding='utf-8') as f:\n",
        "    documents = json.load(f)\n",
        "\n",
        "# Шаг 2: Примеры новых документов для добавления\n",
        "new_docs = [\n",
        "    {\n",
        "        #\"title\": \"Как создать Docker-образ из Dockerfile?\",\n",
        "        \"content\": \"Для создания Docker-образа необходимо использовать команду docker build. Убедитесь, что у вас есть Dockerfile в корне проекта...\"\n",
        "    },\n",
        "    {\n",
        "        #\"title\": \"Работа с ветками в GitHub\",\n",
        "        \"content\": \"Вы можете создавать, переименовывать и удалять ветки в вашем репозитории. Это позволяет работать над разными функциями параллельно и удобно внедрять pull requests...\"\n",
        "    },\n",
        "    {\n",
        "        #\"title\": \"Телеграм-бот: добавление кнопок\",\n",
        "        \"content\": \"Чтобы добавить кнопки в Telegram-бота, используйте ReplyKeyboardMarkup или InlineKeyboardMarkup из библиотеки python-telegram-bot...\"\n",
        "    },\n",
        "    {\n",
        "        #\"title\": \"Оконные функции в PostgreSQL\",\n",
        "        \"content\": \"Оконные функции позволяют выполнять агрегатные вычисления поверх набора строк, разбитого на окна. Примеры функций: ROW_NUMBER(), RANK(), DENSE_RANK(), LEAD(), LAG()...\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Шаг 3: Добавление новых документов в основной список\n",
        "documents.extend(new_docs)\n",
        "\n",
        "# Шаг 4: Сохранение обновлённого списка в файл\n",
        "with open('documents.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(documents, f, ensure_ascii=False, indent=2)"
      ],
      "metadata": {
        "id": "JEfY0Nf1O7UM"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval = RAGEval('documents.json', 'questions.json', retrieval_limit=5, similarity_threshold=0.6)\n",
        "rag_score, useless_docs_idx, questions_wo_docs_idx = eval.evaluate()\n",
        "\n",
        "print('RAG Score:', rag_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gcMrFdnbO_96",
        "outputId": "a445d511-3c6c-4131-fb07-685e6a907b31"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RAG Score: 0.9405951587830335\n"
          ]
        }
      ]
    }
  ]
}