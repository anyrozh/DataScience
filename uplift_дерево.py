# -*- coding: utf-8 -*-
"""Uplift-дерево.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/183Ol0A3z-VlIrojbDHtfqKcybEccddqf

**DeltaDeltaP**
"""

Build(вершина):
    Если вершина на максимальной глубине, то останавливаемся.

    Перебираем все признаки:
        Перебираем “возможные” (определение см. дальше в задаче) варианты порога:
            Разделяем данные по (признак, порог) -> данные_слева, данные_справа
            Если разбиение не удовлетворяет условиям на минимальное количество объектов:
                Continue # т.е. Не рассматриваем это разбиение
            Считаем критерий delta_delta_p

    Выбираем наилучшее по значению критерия разбиение.
    По лучшему разбиению создаем левую вершину и правую вершину.
    Build(левая вершина)
    Build(правая вершина)

В данном задании модель будет оцениваться с помощью метрики uplift@k. Как считается метрика:

1. Сортируем всех пользователей по предсказанному uplift по убыванию и берем топ, например, топ 20%.
2. Считаем средний таргет всех представителей целевой группы из отобранного топа. И считаем средний таргет всех представителей контрольной группы из отобранного топа.
3. Вычисляем разницу между средними таргетами целевой и контрольной группы. Это и есть uplift@k

"""Uplift@k metric."""
from sklearn.utils.validation import check_consistent_length

import numpy as np


def uplift_at_k(y_true, uplift, treatment, k=0.3):
    """Compute uplift at first k observations by uplift of the total sample."""
    check_consistent_length(y_true, uplift, treatment)
    y_true, uplift, treatment = (
        np.array(y_true),
        np.array(uplift),
        np.array(treatment),
    )

    order = np.argsort(uplift, kind="mergesort")[::-1]
    k = int(len(y_true) * k) if isinstance(k, float) else k

    score_ctrl = y_true[order][:k][treatment[order][:k] == 0].mean()
    score_trmnt = y_true[order][:k][treatment[order][:k] == 1].mean()

    return score_trmnt - score_ctrl

import numpy as np
from typing import Optional
from dataclasses import dataclass

@dataclass
class Node:
    idxs: np.ndarray
    depth: int
    ate: float
    is_leaf: bool = False
    split_feat: Optional[int] = None
    split_threshold: Optional[float] = None
    left: Optional['Node'] = None
    right: Optional['Node'] = None

class UpliftTreeRegressor:
    def __init__(
        self,
        max_depth=3,
        min_samples_leaf=1000,
        min_samples_leaf_treated=300,
        min_samples_leaf_control=300,
        feature_names=None
    ):
        self.max_depth = max_depth
        self.min_samples_leaf = min_samples_leaf
        self.min_samples_leaf_treated = min_samples_leaf_treated
        self.min_samples_leaf_control = min_samples_leaf_control
        self.root = None
        self.feature_names = feature_names # for pretty print

    def fit(self, X: np.ndarray, treatment: np.ndarray, y: np.ndarray):
        # Save for prediction
        self.X = X
        self.treatment = treatment
        self.y = y
        self.n_samples, self.n_features = X.shape
        idxs = np.arange(self.n_samples)
        self.root = self._build_tree(idxs, depth=0)
        return self

    def _ate(self, idxs):
        # uplift = E[Y|T=1] - E[Y|T=0]
        T = self.treatment[idxs]
        Y = self.y[idxs]
        treated = T == 1
        control = T == 0
        if np.sum(treated) == 0 or np.sum(control) == 0:
            return 0.0  # if some node has not both, 0 uplift
        tau = np.mean(Y[treated]) - np.mean(Y[control])
        return tau

    def _get_thresholds(self, col):
        unique_values = np.unique(col)
        if len(unique_values) > 10:
            percentiles = np.percentile(
                col, [3, 5, 10, 20, 30, 50, 70, 80, 90, 95, 97]
            )
        else:
            percentiles = np.percentile(unique_values, [10, 50, 90])
        threshold_options = np.unique(percentiles)
        return threshold_options

    def _build_tree(self, idxs, depth):
        node_ate = self._ate(idxs)
        node = Node(
            idxs=idxs,
            depth=depth,
            ate=node_ate
        )
        # Early stop: max depth
        if depth >= self.max_depth:
            node.is_leaf = True
            return node
        # Try all (feat, threshold)
        best_delta = -np.inf
        best_feat = None
        best_thresh = None
        best_left = None
        best_right = None

        for feat_id in range(self.n_features):
            col = self.X[idxs, feat_id]
            thresholds = self._get_thresholds(col)
            for thresh in thresholds:
                left = idxs[col <= thresh]
                right = idxs[col > thresh]
                # Minimal leaf size
                if len(left) < self.min_samples_leaf or len(right) < self.min_samples_leaf:
                    continue
                # Both left/right must have min samples for treated and for control
                l_treat = np.sum(self.treatment[left] == 1)
                l_ctrl = np.sum(self.treatment[left] == 0)
                r_treat = np.sum(self.treatment[right] == 1)
                r_ctrl = np.sum(self.treatment[right] == 0)
                if l_treat < self.min_samples_leaf_treated or l_ctrl < self.min_samples_leaf_control:
                    continue
                if r_treat < self.min_samples_leaf_treated or r_ctrl < self.min_samples_leaf_control:
                    continue
                # delta_delta_p: maximizing |tau_left - tau_right|
                tau_left = self._ate(left)
                tau_right = self._ate(right)
                delta = abs(tau_left - tau_right)
                if delta > best_delta:
                    best_delta = delta
                    best_feat = feat_id
                    best_thresh = thresh
                    best_left = left
                    best_right = right

        if best_feat is None:
            node.is_leaf = True
            return node

        node.split_feat = best_feat
        node.split_threshold = best_thresh

        # Recursive build
        node.left = self._build_tree(best_left, depth + 1)
        node.right = self._build_tree(best_right, depth + 1)
        return node

    def _predict_one(self, x):
        node = self.root
        while not node.is_leaf:
            if x[node.split_feat] <= node.split_threshold:
                node = node.left
            else:
                node = node.right
        return node.ate

    def predict(self, X: np.ndarray) -> np.ndarray:
        # Predict uplift for each observation
        return np.array([self._predict_one(x) for x in X])

    # возможность вывести красивое дерево как в example_tree.txt
    def print_tree(self, node=None, depth=0):
        if node is None:
            node = self.root
        prefix = "  " * depth
        n_items = len(node.idxs)
        print(f"{prefix}{'Root' if depth==0 else ('Left' if node is node.parent.left else 'Right')}{' <leaf>' if node.is_leaf else ''}")
        print(f"{prefix}n_items: {n_items}")
        print(f"{prefix}ATE: {node.ate}")
        print(f"{prefix}split_feat: {self.feature_names[node.split_feat] if ((node.split_feat is not None) and self.feature_names) else node.split_feat}")
        print(f"{prefix}split_threshold: {node.split_threshold}")
        if not node.is_leaf:
            node.left.parent = node
            node.right.parent = node
            self.print_tree(node.left, depth+1)
            self.print_tree(node.right, depth+1)

import pandas as pd
data = pd.read_csv("/content/1716497_2025_08_03.csv")
data.columns

import pandas as pd
import numpy as np

# Получить DataFrame data с помощью ClickHouse запроса, см. выше

feature_cols = ["feat0", "feat1", "feat2", "feat3", "feat4"]
X = data[feature_cols].to_numpy()
y = data["target"].to_numpy()
treatment = data["treatment"].to_numpy()

print("X shape:", X.shape)
print("y shape:", y.shape)
print("treatment shape:", treatment.shape)

model = UpliftTreeRegressor(
    max_depth=3,
    min_samples_leaf=6000,
    min_samples_leaf_treated=2500,
    min_samples_leaf_control=2500,
    feature_names=['feat0', 'feat1', 'feat2', 'feat3', 'feat4']
)
model.fit(X, treatment, y)
uplift_preds = model.predict(X)

model.print_tree()

Root
n_items: 40000
ATE: 163.6276780015877
split_feat: feat0
split_threshold: 0.8428841778032793

	Left
	n_items: 32000
	ATE: 92.4210684009924
	split_feat: feat0
	split_threshold: -0.982722456735287

		Left <leaf>
		n_items: 6400
		ATE: -139.95838047140154
		split_feat: None
		split_threshold: 0

		Right
		n_items: 25600
		ATE: 149.90221050493
		split_feat: feat1
		split_threshold: 0.5254066738679113

			Left <leaf>
			n_items: 17920
			ATE: 95.1193881016224
			split_feat: None
			split_threshold: 0

			Right <leaf>
			n_items: 7680
			ATE: 282.6153675565066
			split_feat: None
			split_threshold: 0

	Right <leaf>
	n_items: 8000
	ATE: 444.7676808824675
	split_feat: None
	split_threshold: 0