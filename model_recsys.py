# -*- coding: utf-8 -*-
"""MODEL-RecSys.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mRQbr4GaCRMsf37j0CLBzRqFfP0Ib8Fa

Ваша задача — **разработать систему, которая сможет предсказывать, какой товар пользователь добавит в корзину следующим.**

Один из важных подходов в этой области — Next Basket Recommendation (NBR). Этот метод предсказывает следующий товар, который пользователь, скорее всего, добавит в корзину. Он помогает сделать процесс покупок более удобным и персонализированным.

**EASE (Embarrassingly Shallow Autoencoders for Sparse Data)** — это простая, но мощная модель рекомендательной системы, которая строит предсказания на основе матрицы взаимодействий корзин с товарами. Основная идея модели заключается в построении матрицы весов **W, где каждый элемент w(ij) показывает, насколько товар i связан с товаром j.**

**Модель обучается минимизировать ошибку между реальной матрицей взаимодействий и предсказанной, при этом накладывая регуляризацию, чтобы избежать переобучения.**

Как это работает?

1. Матрица взаимодействий:
Мы имеем матрицу RRR, где строки — это корщизы, а столбцы — товары. Если товар был добавлен в корзину, в соответствующей ячейке будет значение 1, иначе — 0.
2. Решение линейного уравнения: Модель строит матрицу весов WWW,где каждый элемент wij насколько товар i связан с товаром j.
3. Регуляризация: Добавляется штраф за большие веса, чтобы избежать переобучения.

Матрица взаимодействий
"""

import pandas as pd
df = pd.read_csv("/content/basket_item_dataset - basket_item_dataset.csv")
df['timestamp'] = pd.to_datetime(df['timestamp'])
basket_item_matrix = df.pivot_table(
    index='basket_id', columns='item_id', values='timestamp', aggfunc='count', fill_value=0
).values
basket_item_matrix

df

"""Сначала создается матрица взаимодействий, где 1 означает покупку товара;
Модель обучается, вычисляя матрицу весов W;
Для рекомендаций:

- Делается предсказание по всем товарам

- Уже купленные товары исключаются (им присваивается -inf)

- Выбираются top_k товаров с наивысшими оценками
"""

import numpy as np

class EASE:
    def __init__(self, lambda_reg):
        """
        Инициализация модели EASE.
        Args:
            lambda_reg (float): Параметр регуляризации.
        """
        self.lambda_reg = lambda_reg
        self.W = None  # Матрица весов

    def fit(self, basket_item_matrix):
        """
        Обучение модели EASE.
        Args:
            basket_item_matrix (numpy.ndarray): Матрица взаимодействий (baskets x items).
        """
        if not isinstance(basket_item_matrix, np.ndarray):
            raise ValueError("Матрица basket_item_matrix должна быть numpy.ndarray")
        if basket_item_matrix.ndim != 2:
            raise ValueError("Матрица basket_item_matrix должна быть двумерной")

        # 1. Граммовая матрица (сходство): G = X^T X, shape (n_items, n_items)
        G = basket_item_matrix.T @ basket_item_matrix

        # 2. Добавляем регуляризацию на диагональ: G_ii += lambda
        #Что делаем: К диагональным элементам G прибавляем lambda_reg.
        #Зачем: Регуляризация предотвращает "саморекомендации" (чтобы товар не рекомендовал сам себя).
        #Чем больше lambda_reg, тем слабее связи между товарами (уменьшает переобучение).
        diag_indices = np.diag_indices(G.shape[0])
        G[diag_indices] += self.lambda_reg

        # 3. Инвертируем эту матрицу
        P = np.linalg.inv(G)

        # 4. Вычисляем матрицу весов W: W = -P / diag(P)
        W = -P / np.diag(P)[:, None]

        # 5. Диагональ W выставляем в 0 (не рекомендуем то, что уже есть)
        W[diag_indices] = 0.0
        self.W = W

    def predict(self, basket_item_matrix):
        """
        Предсказание для всех корзин.
        Args:
            basket_item_matrix (numpy.ndarray): Матрица взаимодействий (baskets x items).
        Returns:
            numpy.ndarray: Матрица предсказаний (baskets x items).
        """
        if self.W is None:
            raise ValueError("Модель не обучена. Сначала вызовите fit().")
        # shape: (n_baskets, n_items) @ (n_items, n_items) => (n_baskets, n_items)
        return basket_item_matrix @ self.W

    def recommend(self, basket_item_matrix, basket_index, top_k=5):
        """
        Рекомендации для конкретной корзины.
        Args:
            basket_item_matrix (numpy.ndarray): Матрица взаимодействий (baskets x items).
            basket_index (int): Индекс корзины.
            top_k (int): Количество рекомендаций.
        Returns:
            list: Список индексов рекомендованных товаров.
        """
        if basket_index < 0 or basket_index >= basket_item_matrix.shape[0]:
            raise ValueError(f"Индекс корзины должен быть от 0 до {basket_item_matrix.shape[0] - 1}")
        if self.W is None:
            raise ValueError("Модель не обучена. Сначала вызовите fit().")

        # Получаем предсказанные оценки для корзины
        scores = self.predict(basket_item_matrix)[basket_index].copy()  # shape (n_items,)
        # Уже купленные (интеракция 1) товары зануляем
        scores[basket_item_matrix[basket_index] == 1] = -np.inf
        # Сортируем по убыванию и берём top_k
        recommended_indices = np.argpartition(scores, -top_k)[-top_k:]
        # Пересортировать в точном порядке убывания
        recommended_indices = recommended_indices[np.argsort(scores[recommended_indices])[::-1]]
        return recommended_indices.tolist()

"""**TIFU-KNN**

**TIFU-KNN (Time-Weighted Item Frequency User-K Nearest Neighbors)** — это модель, которая помогает учитывать как частоту, так и временные аспекты взаимодействий пользователей с товарами. Это особенно полезно в сценариях, где пользователи совершают повторяющиеся действия, такие как регулярные покупки.

TIFU-KNN решает эту проблему, учитывая:

1. **Частотность действий:** Учитывается, насколько часто пользователь взаимодействует с конкретным товаром. Частотность нормализуется, чтобы избежать перекоса в сторону товаров, с которыми взаимодействуют все пользователи.
2. **Временные веса:** Новые взаимодействия с товаром имеют больший вес, чем старые.
3. **User-KNN:** Используется ближайший сосед пользователя на основе похожести профилей, чтобы учитывать персональные предпочтения.

Основные шаги работы TIFU-KNN:

1. **Сбор данных:** Создаётся матрица взаимодействий (user × item), где указано количество взаимодействий пользователя с каждым товаром.
2. **Нормализация частоты**: Количество взаимодействий нормализуется, чтобы избежать перекоса в сторону популярных товаров.
2. **Добавление временных весов:** Новые взаимодействия получают больший вес.
3. **Сравнение профилей:** Вычисляется схожесть между пользователями для нахождения ближайших соседей.
4. **Формирование рекомендаций:** На основе профиля ближайших соседей и временной частотности формируется список товаров для рекомендации.
"""

from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

class TIFUKNN:
    def __init__(self, k=5, time_decay_factor=0.9):
        self.k = k
        self.time_decay_factor = time_decay_factor
        self.basket_item_matrix = None
        self.basket_index_map = None
        self.item_index_map = None

    def fit(self, interactions: pd.DataFrame):
        # Создаем маппинги для корзин и товаров
        baskets = interactions['basket_id'].unique()
        items = interactions['item_id'].unique()

        self.basket_index_map = {basket: idx for idx, basket in enumerate(baskets)}
        self.item_index_map = {item: idx for idx, item in enumerate(items)}

        # Инициализируем матрицу (корзины × товары)
        self.basket_item_matrix = pd.DataFrame(
            np.zeros((len(baskets), len(items))),
            index=baskets,
            columns=items
        )
        current_time = datetime.now()

        # Заполняем матрицу с временными весами
        for _, row in interactions.iterrows():
            basket = row['basket_id']
            item = row['item_id']
            time_weight = self._compute_time_weight(current_time, row['timestamp'])
            self.basket_item_matrix.at[basket, item] += time_weight

        # Нормализуем по строкам (корзинам)
        row_sums = self.basket_item_matrix.sum(axis=1)
        self.basket_item_matrix = self.basket_item_matrix.div(row_sums, axis=0).fillna(0)

    def _compute_time_weight(self, current_time, interaction_time):
        delta = (current_time - interaction_time).days
        return self.time_decay_factor ** delta

    def recommend(self, basket_id, top_n=5):
        if basket_id not in self.basket_index_map:
            raise ValueError(f"Basket {basket_id} not found in training data.")

        # Получаем вектор текущей корзины
        basket_vector = self.basket_item_matrix.loc[basket_id].values.reshape(1, -1)

        # Считаем косинусную схожесть со всеми корзинами
        similarities = cosine_similarity(basket_vector, self.basket_item_matrix.values).flatten()

        # Находим k ближайших соседей (исключая текущую корзину)
        basket_idx = self.basket_index_map[basket_id]
        similarities[basket_idx] = -1  # Исключаем текущую корзину
        neighbors_idx = np.argsort(similarities)[-self.k:]
        neighbors = self.basket_item_matrix.iloc[neighbors_idx]

        # Суммируем веса соседей для рекомендаций
        recommendations = neighbors.sum(axis=0)

        # Исключаем товары, уже есть в корзине
        basket_items = self.basket_item_matrix.loc[basket_id] > 0
        recommendations[basket_items] = -1

        # Возвращаем top_n рекомендаций
        return recommendations.nlargest(top_n).index.tolist()

# Инициализация и обучение модели
tifu_knn = TIFUKNN(k=2, time_decay_factor=0.9)  # Берем 2 ближайших соседа для наглядности
tifu_knn.fit(df)

# Тестируем рекомендации для корзины 1
print("Рекомендации для корзины 1:", tifu_knn.recommend(basket_id=1, top_n=3))

"""**BERT4Rec**

BERT4Rec — это модель, основанная на трансформерах, которая используется для рекомендаций. Она вдохновлена архитектурой BERT (Bidirectional Encoder Representations from Transformers).  Как и BERT в NLP, BERT4Rec использует механизм самовнимания (self-attention) для анализа последовательностей. **Здесь последовательности представляют собой временные взаимодействия пользователей с товарами.**

Модель **предсказывает следующий товар**, анализируя всю последовательность взаимодействий как единое целое. Это позволяет учитывать сложные временные и логические зависимости между товарами, а также обрабатывать пропуски и нетипичные паттерны взаимодействий.

**Основные идеи:**
1. Маскировка входной последовательности: В процессе обучения часть элементов последовательности заменяется специальными токенами [MASK]. Модель обучается восстанавливать скрытые элементы, анализируя связи между остальными токенами.
2. **Двусторонний контекст:** Модель рассматривает всю последовательность как в прямом, так и в обратном порядке, что позволяет учитывать все связи между элементами.
3. **Обучение на последовательностях:** Каждая последовательность представляет собой историю взаимодействий одного пользователя.
"""

import torch
from torch import nn
from transformers import BertConfig, BertModel

class BERT4Rec(nn.Module):
    def __init__(self, vocab_size, bert_config, add_head=True,
                 tie_weights=True, padding_idx=0, init_std=0.02):
        """
        Инициализация модели BERT4Rec.

        Args:
            vocab_size (int): Размер словаря (количество уникальных товаров + специальные токены).
            bert_config (dict): Конфигурация BERT (например, размер скрытого слоя, количество голов внимания).
            add_head (bool): Добавлять ли выходной слой для предсказания.
            tie_weights (bool): Связывать ли веса эмбеддингов и выходного слоя.
            padding_idx (int): Индекс для паддинга.
            init_std (float): Стандартное отклонение для инициализации весов.
        """
        super().__init__()
        self.vocab_size = vocab_size
        self.bert_config = bert_config
        self.add_head = add_head
        self.tie_weights = tie_weights
        self.padding_idx = padding_idx
        self.init_std = init_std

        # Получаем hidden_size из конфига (работает и для dict, и для BertConfig)
        hidden_size = bert_config['hidden_size'] if isinstance(bert_config,dict) else bert_config.hiiden

        # TODO: Создайте слой эмбеддингов
        self.embed_layer = nn.Embedding(
            num_embeddings=vocab_size,
            embedding_dim=hidden_size,
            padding_idx=padding_idx
        )  # YOUR CODE HERE: Используйте nn.Embedding

        # TODO: Создайте модель Transformer на основе конфигурации BERT
        config = bert_config
        self.transformer_model = config  # YOUR CODE HERE: Используйте BertModel

        # TODO: Добавьте выходной слой (если add_head=True)
        if self.add_head:
            self.head = nn.Linear(hidden_size,vocab_size)  # YOUR CODE HERE: Используйте nn.Linear
            if self.tie_weights:
              self.head.weight = self.embed_layer.weight
                # YOUR CODE HERE: Свяжите веса выходного слоя с эмбеддингами

        # TODO: Инициализируйте веса
        self.init_weights()

    def init_weights(self):
        """
        Инициализация весов эмбеддингов.
        """
        # TODO: Инициализируйте веса эмбеддингов нормальным распределением
        nn.init.normal_(self.embed_layer.weight, mean=0.0, std=self.init_std)
        # YOUR CODE HERE: Используйте normal_ для инициализации
        if self.padding_idx is not None:
          with torch.no_grad():
                self.embed_layer.weight[self.padding_idx].fill_(0)

    def forward(self, input_ids, attention_mask):
        """
        Прямой проход модели.

        Args:
            input_ids (torch.Tensor): Тензор с ID товаров (batch_size x seq_len).
            attention_mask (torch.Tensor): Маска для игнорирования паддинга.

        Returns:
            torch.Tensor: Логиты для всех товаров.
        """
        # TODO: Получите эмбеддинги для input_ids
        embeds = self.embed_layer(input_ids)  # YOUR CODE HERE: Используйте self.embed_layer

        # TODO: Пропустите эмбеддинги через Transformer
        transformer_outputs = self.transformer_model(inputs_embeds=embeds,
            attention_mask=attention_mask)# YOUR CODE HERE: Используйте self.transformer_model

        # TODO: Получите последний скрытый слой
        outputs =  transformer_outputs.last_hidden_state  # YOUR CODE HERE: Используйте last_hidden_state

        # TODO: Пропустите выход через головной слой (если add_head=True)
        if self.add_head:
            outputs = self.head(outputs)  # YOUR CODE HERE: Используйте self.head

        return outputs

"""**Гибридная рекомендательная система**

1. **EASE отлично справляется с обработкой больших объемов данных и предоставляет базовые рекомендации**, но страдает от низкой персонализации.
2. TIFU-KNN **превосходно работает для повторяющихся действий и часто используемых товаров**, но ограничена в способности понимать новые или сложные шаблоны поведения пользователей.
3. BERT4Rec — современная нейросеть, которая способна учитывать **сложные последовательности действий, однако требует значительных вычислительных ресурсов.**

**Комбинирование этих моделей позволяет использовать их сильные стороны одновременно.** Такой подход называется гибридной рекомендацией. Цель гибридных систем — создавать персонализированные рекомендации, сохраняя высокую производительность.
"""