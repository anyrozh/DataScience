{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Этап 1:**\n",
        "\n",
        "1. Прогноз поставок\n",
        "\n",
        "2. Цель: обеспечить точную подачу товаров на склады маркетплейса с учётом спроса, сезонности, цен, логистики и ограничений.\n",
        "\n",
        "3. Функциональность:\n",
        "  - Построение прогноза спроса по каждому артикулу по дням (горизонт: 7–30 дней)\n",
        "  - Учет сезонных факторов, акций, календарных праздников, промо, выходных\n",
        "  - Слежение за остатками, вычисление даты исчерпания при текущем спросе.\n",
        "  - Учет складских ограничений по API Ozon (доступность мест, FBO-зоны).\n",
        "  - Расчёт рекомендованного объёма поставки.\n",
        "  - Экспорт таблиц в Excel/CSV для логистики.\n",
        "\n",
        "4. Источники данных:\n",
        " - История продаж за 12+ месяцев (по дням).\n",
        " - Остатки по каждому SKU.\n",
        " - API Ozon: остатки, движение товаров, доступность поставок.\n",
        " - Календарь акций и праздников (вводится вручную).\n",
        " - Информация по складам и лимитам (из API или вручную)."
      ],
      "metadata": {
        "id": "KkBhBZ_fSE3y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "В архитектуре TFT можно выделить следующие блоки:\n",
        "\n",
        "1. **Static Covariate Encoders** - обработка статических (неизменяемых) признаков.\n",
        "Помощь модели в учете постоянных особенностей объектов\n",
        "2. **Variable Selection** - отбор переменных признаков. Избавляется от шумных признаков, которые могут мешать модели\n",
        "3. **LSTM** - для краткосрочных паттернов. Обрабатывает локальные временные зависимости, улавливает недавние тренды и сезонность\n",
        "4. **Interpretable Multi-head Attention** - для долгосрочных паттернов. Объединяет информацию из разных временных масштабов, находит сложные долгосрочные зависимости в данных\n",
        "5. **Quantile Outputs** - прогнозирование квантилей. Оценивает неопределенность предсказаний через различные квантили, позволяет понять возможный разброс прогнозируемых значений"
      ],
      "metadata": {
        "id": "eRiQqUhMSeNc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Статичные признаки проходят следующий путь:**\n",
        "\n",
        "1) **Категориальные данные преобразуются в эмбеддинги**, а числовые данные используются как есть\n",
        "\n",
        "2) Все **статичные признаки проходят через VSN слой**\n",
        "\n",
        "3) Затем создаются **контекстные вектора для передачи в последующие слои модели**\n",
        "\n"
      ],
      "metadata": {
        "id": "a4Wjv9qrXScS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1.   с(s) - Основной контекст статичных признаков - отбор признаков\n",
        "2.   c(c) - Начальное состояние cell state для LSTM\n",
        "3.   c(h) - Начальное hidden state для LSTM - поиск краткосрочных паттернов\n",
        "4.   c(e) - Обогащает выходы LSTM перед слоем трансформеров - поиск долгосрочных паттернов\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AEK50rSbXsks"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Локальный прогноз (LSTM)**\n",
        "\n",
        "В архитектуре Temporal Fusion Transformer обработка временных зависимостей происходит на двух уровнях: локальном и глобальном. Такой двойной подход позволяет модели эффективно улавливать как краткосрочные паттерны, так и долгосрочные взаимосвязи в данных.\n",
        "\n",
        "На локальном уровне используется уже знакомая нам  LSTM (Long Short-Term Memory). Представьте, что вы анализируете продажи в магазине: **LSTM отслеживает последовательные изменения в данных, например, как продажи меняются от дня к дню или от недели к неделе. Эта часть модели справляется с выявлением краткосрочных зависимостей и локальных паттернов.**\n",
        "\n",
        "Механизм фильтрации (gate) и skip connections после LSTM позволяют игнорировать **неиспользуемые компоненты архитектуры, адаптируя глубину и сложность сети для работы с различными наборами данных и сценариями.**"
      ],
      "metadata": {
        "id": "vVPo94MGZmnq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "В отличие от LSTM, **Multi-head attention способен напрямую связывать события, происходящие в разные моменты времени, даже если они разделены большими промежутками.** Это похоже на то, как опытный аналитик может заметить, что всплеск продаж в декабре этого года похож на прошлогодний."
      ],
      "metadata": {
        "id": "RssZfoluaCt1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Механизм внимания особенно **полезен для выявления сезонности, долгосрочных трендов и повторяющихся паттернов. Например, он может обнаружить, что пик продаж определенных товаров наблюдается каждый год перед началом учебного года, или что существует устойчивая корреляция между продажами разных категорий товаров.**\n",
        "\n",
        "Рассмотрим пример, представьте, что вы анализируете продажи магазина:\n",
        "\n",
        "- Первая голова замечает, что продажи выше по выходным\n",
        "\n",
        "- Вторая отслеживает рост в начале каждого месяца (после зарплаты)\n",
        "\n",
        "- Третья фокусируется на сезонных пиках (например, перед Новым годом)"
      ],
      "metadata": {
        "id": "mhmSEjCQau_t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Суть attention в том, чтобы научить модель фокусироваться на важных частях входных данных при генерации выхода. По сути, *attention позволяет модели динамически создавать связи между разными частями входных данных*, *вместо того чтобы работать с фиксированным контекстом*, как это делает LSTM.**"
      ],
      "metadata": {
        "id": "_l_kVNLGaN-7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3jZnsmElW5_f"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from typing import Optional, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class CustomLSTM(nn.Module):\n",
        "    def __init__(self, input_size: int, hidden_size: int):\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Forget gate parameters\n",
        "        self.weight_forget_x = nn.Parameter(torch.Tensor(input_size, hidden_size))\n",
        "        self.weight_forget_h = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
        "        self.bias_forget = nn.Parameter(torch.Tensor(hidden_size))\n",
        "\n",
        "        # Input gate parameters\n",
        "        self.weight_input_x = nn.Parameter(torch.Tensor(input_size, hidden_size))\n",
        "        self.weight_input_h = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
        "        self.bias_input = nn.Parameter(torch.Tensor(hidden_size))\n",
        "\n",
        "        # Cell gate parameters\n",
        "        self.weight_cell_x = nn.Parameter(torch.Tensor(input_size, hidden_size))\n",
        "        self.weight_cell_h = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
        "        self.bias_cell = nn.Parameter(torch.Tensor(hidden_size))\n",
        "\n",
        "        # Output gate parameters\n",
        "        self.weight_output_x = nn.Parameter(torch.Tensor(input_size, hidden_size))\n",
        "        self.weight_output_h = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
        "        self.bias_output = nn.Parameter(torch.Tensor(hidden_size))\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self) -> None:\n",
        "        \"\"\"Initialize weights using uniform distribution.\"\"\"\n",
        "        stdv = 1.0 / math.sqrt(self.hidden_size)\n",
        "        for weight in self.parameters():\n",
        "            weight.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x: torch.Tensor,\n",
        "        initial_states: Optional[Tuple[torch.Tensor, torch.Tensor]] = None\n",
        "    ) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
        "        \"\"\"\n",
        "        Forward pass of LSTM.\n",
        "\n",
        "        Args:\n",
        "            x: Input tensor of shape (batch_size, sequence_length, input_size)\n",
        "            initial_states: Tuple of initial hidden and cell states\n",
        "                          Each of shape (batch_size, hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            tuple: (output_sequence, (final_hidden_state, final_cell_state))\n",
        "                  output_sequence: tensor of shape (batch_size, sequence_length, hidden_size)\n",
        "                  final_hidden_state: tensor of shape (batch_size, hidden_size)\n",
        "                  final_cell_state: tensor of shape (batch_size, hidden_size)\n",
        "        \"\"\"\n",
        "        batch_size, sequence_length, _ = x.size()\n",
        "        hidden_sequence = []\n",
        "\n",
        "        if initial_states is None:\n",
        "            hidden_state = torch.zeros(batch_size, self.hidden_size).to(x.device)\n",
        "            cell_state = torch.zeros(batch_size, self.hidden_size).to(x.device)\n",
        "        else:\n",
        "            hidden_state, cell_state = initial_states\n",
        "\n",
        "        for t in range(sequence_length):\n",
        "            curr_x = x[:, t, :]\n",
        "\n",
        "            # Gates computation\n",
        "\n",
        "            #ШАГ 1. Первый шаг в LSTM – определить, какую информацию можно выбросить из состояния ячейки.\n",
        "            #Если вес равен 1, то предыдущие состояние полностью сохранится, а если forget layer gate вернул 0, то предыдущее состояние будет \"забыто\".\n",
        "\n",
        "            \"\"\"\n",
        "            Для задачи предсказания временных рядов, можно представить, что модель перестает учитывать новогодние праздники, потому что требуется прогноз на середину января.\n",
        "            \"\"\"\n",
        "            forget_gate = torch.sigmoid(curr_x @ self.weight_forget_x + hidden_state @ self.weight_forget_h + self.bias_forget)\n",
        "\n",
        "            #ШАГ 2. решить, какая новая информация будет храниться в состоянии текущей ячейки. Этот этап состоит из двух частей.\n",
        "            #Сначала сигмоидальный слой входного фильтра (input layer gate) определяет, какие значения следует обновить.\n",
        "            #Затем tanh-слой строит вектор C(t)^~ кандидатов нового состояния, которые потом могут быть к состоянию ячейки C(t).\n",
        "\n",
        "            #Получение вектора состояния\n",
        "            \"\"\"\n",
        "            Можно представить, как модель замечает замедление тренда у временного ряда.\n",
        "            \"\"\"\n",
        "            input_gate = torch.sigmoid(curr_x @ self.weight_input_x + hidden_state @ self.weight_input_h + self.bias_input)\n",
        "\n",
        "            #ШАГ 3. Теперь очередь обновить старое состояние ячейки. Из C(t-1) в C(t)\n",
        "\n",
        "            \"\"\"\n",
        "            Тут модель может забыть про новогодние праздники и делать прогнозы на обычные дни и с учетом замедленного тренда.\n",
        "            \"\"\"\n",
        "\n",
        "            cell_candidate = torch.tanh(curr_x @ self.weight_cell_x + hidden_state @ self.weight_cell_h + self.bias_cell)\n",
        "\n",
        "            #ШАГ 4.\n",
        "            #Остается решить, что мы собираемся выводить в качестве скрытого состояния для следующего блока LSTM.\n",
        "\n",
        "            \"\"\"\n",
        "            Для задачи прогноза временного ряда модель может передать дальше информацию о снижении тренда.\n",
        "            \"\"\"\n",
        "\n",
        "            output_gate = torch.sigmoid(curr_x @ self.weight_output_x + hidden_state @ self.weight_output_h + self.bias_output)\n",
        "\n",
        "            # States update\n",
        "            cell_state = forget_gate * cell_state + input_gate * cell_candidate\n",
        "            hidden_state = output_gate * torch.tanh(cell_state)\n",
        "\n",
        "            hidden_sequence.append(hidden_state.unsqueeze(0))\n",
        "\n",
        "        # Stack and reshape hidden sequence\n",
        "        hidden_sequence = torch.cat(hidden_sequence, dim=0)\n",
        "        hidden_sequence = hidden_sequence.transpose(0, 1).contiguous()\n",
        "\n",
        "        return hidden_sequence, (hidden_state, cell_state)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   residual = x (если размерности совпадают)\n",
        "*   h = ELU(**Linear(x)**) → [0.8, -0.2, 1.5]\n",
        "\n",
        "\n",
        "*   h = Linear(h) → [0.5, 0.3, -0.1]\n",
        "*   h = Dropout(h) → [0.5, 0.3, -0.1]\n",
        "\n",
        "\n",
        "\n",
        "*   h = GLU(h) → [0.5*σ(0.5), 0.3*σ(0.3), -0.1*σ(-0.1)] ≈ [0.31, 0.19, -0.05].\n",
        "*   h = h + residual → [1.31, -0.31, 1.95]\n",
        "\n",
        "\n",
        "x = LayerNorm(h) → Нормализованный выход."
      ],
      "metadata": {
        "id": "LZXedjNf5lyg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gated Residual Network (GRN)  - это блок, который позволяет сети \"решать\" насколько сложные преобразования нужно применить к входным данным, вплоть до полного пропуска преобразований, если они не нужны. Это особенно полезно, когда мы заранее не знаем, какие входные переменные важны.\n"
      ],
      "metadata": {
        "id": "IfWfeNQh71TG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class GatedLinearUnit(nn.Module):\n",
        "    \"\"\"\n",
        "    Gated Linear Unit implementation.\n",
        "\n",
        "    Args:\n",
        "        input_size (int): Size of input features\n",
        "        output_size (int): Size of output features\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Output tensor after applying GLU\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(input_size, output_size)\n",
        "        self.sigmoid = nn.Linear(input_size, output_size)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass of the GLU.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Result of linear transformation multiplied by sigmoid gate\n",
        "        \"\"\"\n",
        "        return self.linear(x) * torch.sigmoid(self.sigmoid(x))\n",
        "\n",
        "\n",
        "class GatedResidualNetwork(nn.Module):\n",
        "    \"\"\"\n",
        "    Gated Residual Network implementation.\n",
        "\n",
        "    Args:\n",
        "        input_size (int): Size of input features\n",
        "        hidden_size (int): Size of hidden layer\n",
        "        dropout_rate (float): Dropout rate for regularization\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Output tensor after processing through GRN\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, hidden_size, dropout_rate):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.elu_dense = nn.Sequential(\n",
        "            nn.Linear(input_size, hidden_size),\n",
        "            nn.ELU()\n",
        "        )\n",
        "\n",
        "        self.linear_dense = nn.Linear(hidden_size, hidden_size)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.glu = GatedLinearUnit(hidden_size, hidden_size)\n",
        "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
        "\n",
        "        self.project = nn.Linear(input_size, hidden_size)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass of the GRN.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Processed tensor\n",
        "\n",
        "         Слой с функций активации ELU\n",
        "         Механизм фильтрации GLU\n",
        "         Residual (skip) connections\n",
        "         Нормализация LayerNorm\n",
        "         Дополнительно используется Dropout после линейного слоя.\n",
        "\n",
        "         Подготовка residual-пути (project или identity).\n",
        "         Основной блок:\n",
        "         ELU → Linear → Dropout → GLU.\n",
        "         Добавление residual-связи и нормализация.\n",
        "        \"\"\"\n",
        "        if x.shape[-1] != self.hidden_size:\n",
        "            residual = self.project(x)\n",
        "        else:\n",
        "            residual = x\n",
        "\n",
        "        # слой активации ELU\n",
        "        # ELU transformation\n",
        "        h = self.elu_dense(x)\n",
        "\n",
        "        # Linear transformation\n",
        "        h = self.linear_dense(h)\n",
        "\n",
        "        # Apply dropout\n",
        "        h = self.dropout(h)\n",
        "\n",
        "        # Apply GLU gating mechanism\n",
        "        h = self.glu(h)\n",
        "\n",
        "        # Add residual connection\n",
        "        h = h + residual\n",
        "\n",
        "        # Apply layer normalization\n",
        "        x = self.layer_norm(h)\n",
        "        return x"
      ],
      "metadata": {
        "id": "GMIp9ssGX6h8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Variable Selection Network (VSN) - определяет важность (выбирает) каждого признака, **помогая модели сфокусироваться на действительно важных фичах.** VSN блоки действуют как умные фильтры, оценивающие важность каждого признака и удаляющие **ненужные-шумные данные которые могут мешать модели.** В TFT это самый первый слой через который проходят входные данные."
      ],
      "metadata": {
        "id": "BpHJ6blR8Nbm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Используя **VSN модель автоматически определяет важность каждого признака.**"
      ],
      "metadata": {
        "id": "UVZC_-l1ZF9B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Представьте, что вы анализируете продажи магазина: одни факторы, например, сезонность или день недели, могут оказывать сильное влияние на продажи, тогда как другие, скажем, влажность воздуха, могут быть менее значимыми."
      ],
      "metadata": {
        "id": "r3UY-KR08bcN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "from typing import List\n",
        "\n",
        "\n",
        "class GatedLinearUnit(nn.Module):\n",
        "    \"\"\"\n",
        "    Gated Linear Unit implementation.\n",
        "\n",
        "    Args:\n",
        "        input_size (int): Size of input features\n",
        "        output_size (int): Size of output features\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Output tensor after applying GLU\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(input_size, output_size)\n",
        "        self.sigmoid = nn.Linear(input_size, output_size)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass of the GLU.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Result of linear transformation multiplied by sigmoid gate\n",
        "        \"\"\"\n",
        "        return self.linear(x) * torch.sigmoid(self.sigmoid(x))\n",
        "\n",
        "\n",
        "class GatedResidualNetwork(nn.Module):\n",
        "    \"\"\"\n",
        "    Gated Residual Network implementation.\n",
        "\n",
        "    Args:\n",
        "        input_size (int): Size of input features\n",
        "        hidden_size (int): Size of hidden layer\n",
        "        dropout_rate (float): Dropout rate for regularization\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Output tensor after processing through GRN\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, hidden_size, dropout_rate):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.elu_dense = nn.Sequential(\n",
        "            nn.Linear(input_size, hidden_size),\n",
        "            nn.ELU()\n",
        "        )\n",
        "\n",
        "        self.linear_dense = nn.Linear(hidden_size, hidden_size)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.glu = GatedLinearUnit(hidden_size, hidden_size)\n",
        "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
        "\n",
        "        self.project = nn.Linear(input_size, hidden_size)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass of the GRN.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Processed tensor\n",
        "        \"\"\"\n",
        "        if x.shape[-1] != self.hidden_size:\n",
        "            residual = self.project(x)\n",
        "        else:\n",
        "            residual = x\n",
        "\n",
        "        x = self.elu_dense(x)\n",
        "        x = self.linear_dense(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.glu(x)\n",
        "        x = x + residual\n",
        "        x = self.layer_norm(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class VariableSelection(nn.Module):\n",
        "    \"\"\"\n",
        "    Variable Selection Network using Gated Residual Networks.\n",
        "\n",
        "    Args:\n",
        "        input_size (int): Size of input features\n",
        "        hidden_size (int): Size of hidden layer\n",
        "        dropout_rate (float): Dropout rate for regularization\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Weighted combination of processed features\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size: int, hidden_size: int, dropout_rate: float) -> None:\n",
        "        super().__init__()\n",
        "        self.num_features = input_size  # number of input features\n",
        "        self.grns = nn.ModuleList()\n",
        "\n",
        "        # Create a GRN for each feature independently\n",
        "        for _ in range(self.num_features):\n",
        "            grn = GatedResidualNetwork(hidden_size, hidden_size, dropout_rate)\n",
        "            self.grns.append(grn)\n",
        "\n",
        "        # Create a GRN for the concatenation of all the features\n",
        "        self.grn_concat = GatedResidualNetwork(hidden_size * self.num_features,\n",
        "                                             hidden_size,\n",
        "                                             dropout_rate)\n",
        "        self.softmax = nn.Linear(hidden_size, self.num_features)\n",
        "\n",
        "    def forward(self, inputs: List[torch.Tensor]) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass of the Variable Selection Network.\n",
        "\n",
        "        Args:\n",
        "            inputs (List[torch.Tensor]): List of input feature tensors\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Weighted combination of processed features\n",
        "        \"\"\"\n",
        "        # Concatenated inputs for features weights\n",
        "        v = torch.cat(inputs, dim=-1)\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "        v = self.grn_concat(v)\n",
        "        v = self.softmax(v)\n",
        "        v = torch.softmax(v, dim=-1)\n",
        "        v = v.unsqueeze(-1)\n",
        "\n",
        "        # Add GRN for each feature\n",
        "        x = []\n",
        "        for idx, input in enumerate(inputs):\n",
        "            # YOUR CODE HERE\n",
        "            out = self.grns[idx](input)\n",
        "            x.append(out)\n",
        "\n",
        "        x = torch.stack(x, dim=1)\n",
        "\n",
        "        # Compute weighted sum\n",
        "        result = torch.matmul(v.transpose(-2, -1), x).squeeze(1)\n",
        "        return result"
      ],
      "metadata": {
        "id": "InuObZX3FhGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обрати внимание, что **каждый шаг временного ряда**, как из исторических данных (past inputs), **так и из данных для будущего (know future inputs), передается в свой отдельный слой VSN.** На последующих схемах мы будем опускать этот момент для упрощения визуализации и восприятия."
      ],
      "metadata": {
        "id": "AVE3WKLLQ9Ww"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hr7I6YEKRFFh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}