# -*- coding: utf-8 -*-
"""t-featured.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zeKSuKMm3IMIdeq7Nf7GLvQyJCWDJMeg

**T-TESTED FEATURES**
"""

import numpy as np
from sklearn.base import clone
from sklearn.model_selection import cross_val_score

try:
    from tqdm import trange
except ImportError:
    trange = range

class SequentialForwardSelector:
    """
    Sequential forward selection.
    Algorithm selects features one by one, each time adding the feature that
    improves the model the most.

    Parameters
    ----------
    model: estimator :
        ML model, e.g. LinearRegression
    cv: cross-validation generator :
        cross-validation generator, e.g. KFold, RepeatedKFold
    max_features: int :
        maximum number of features to select
    verbose: int :
        (Default value = 0)
        verbosity level

    Attributes
    ----------
    n_features_: int :
        number of features in the dataset
    selected_features_: List[int] :
        list of selected features, ordered by index
    n_selected_features_: int :
        number of selected features
    """

    def __init__(
        self,
        model,
        cv,
        max_features: int = 10,
        verbose: int = 0,
    ) -> None:
        self.model = model
        self.cv = cv
        self.max_features = max_features
        self.verbose = verbose
        self.selected_features_ = []
        self.n_features_ = None

    def fit(self, X: np.ndarray, y: np.ndarray) -> None:
        X = np.asarray(X)
        n_samples, n_features = X.shape

        self.n_features_ = n_features

        included = []
        excluded = list(range(n_features))

        iterator = (
            trange(self.max_features, desc='SFS', leave=False)
            if self.verbose else range(self.max_features)
        )

        for _ in iterator:
            best_score = -np.inf
            best_feature = None

            for feat in excluded:
                selected = included + [feat]
                X_selected = X[:, selected]
                scores = cross_val_score(
                    clone(self.model), X_selected, y,
                    cv=self.cv, scoring="r2", n_jobs=-1
                )
                score = scores.mean()

                if score > best_score:
                    best_score = score
                    best_feature = feat

            if best_feature is not None:
                included.append(best_feature)
                excluded.remove(best_feature)
            else:
                break

            if self.verbose:
                print(f"Step {len(included)}/{self.max_features} added feature {best_feature} (score={best_score:.4f})")

        self.selected_features_ = sorted(included)

    def transform(self, X: np.ndarray) -> np.ndarray:
        return X[:, self.selected_features_]

    @property
    def n_selected_features_(self):
        return len(self.selected_features_)

def generate_dataset(
    n_samples: int = 10_000,
    n_features: int = 50,
    n_informative: int = 10,
    random_state: int = 42,
) -> Tuple:
    """
    Generate dataset.

    Parameters
    ----------
    n_samples: int :
        (Default value = 10_000)
        number of samples
    n_features: int :
        (Default value = 50)
        number of features
    n_informative: int :
        (Default value = 10)
        number of informative features, other features are noise
    random_state: int :
        (Default value = 42)
        random state for reproducibility

    Returns
    -------
    X: np.ndarray :
        features
    y: np.ndarray :
        target

    """
    X, y = make_regression(
        n_samples=n_samples,
        n_features=n_features,
        noise=100,
        random_state=random_state,
        n_informative=n_informative,
        bias=100,
        shuffle=True,
    )
    return X, y


def run() -> None:
    """Run."""
    random_state = 42
    n_samples = 10_000
    n_features = 50
    n_informative = 5
    max_features = 10
    n_splits = 3
    n_repeats = 10

    # generate data
    X, y = generate_dataset(n_samples, n_features, n_informative, random_state)

    # define model and cross-validation
    model = LinearRegression()
    cv = RepeatedKFold(
        n_splits=n_splits, n_repeats=n_repeats, random_state=random_state
    )

    # baseline
    scores = cross_val_score(model, X, y, scoring="r2", cv=cv, n_jobs=-1)
    print(f"Baseline features count: {X.shape[1]}")
    print(f"Baseline R2 score: {scores.mean():.4f}")

    selector = SequentialForwardSelector(model, cv, max_features, verbose=1)
    selector.fit(X, y)
    X_transformed = selector.transform(X)
    scores = cross_val_score(model, X_transformed, y, scoring="r2", cv=cv, n_jobs=-1)

    print(f"Features: {selector.selected_features_}")
    print(f"Features count: {selector.n_selected_features_}")
    print(f"Mean R2 score: {scores.mean():.4f}")


if __name__ == "__main__":
    run()

"""**Выбор признаков через t-тест**"""

import numpy as np
from sklearn.base import clone
from sklearn.model_selection import cross_val_score
from scipy.stats import ttest_rel

try:
    from tqdm import trange
except ImportError:
    trange = range

class SequentialForwardSelector:
    """
    Sequential forward selection с t-test для отбора признаков.
    """

    def __init__(
        self,
        model,
        cv,
        max_features: int = 10,
        verbose: int = 0,
        alpha: float = 0.05,
        bonferroni: bool = True,
    ) -> None:
        self.model = model
        self.cv = cv
        self.max_features = max_features
        self.verbose = verbose
        self.alpha = alpha
        self.bonferroni = bonferroni
        self.selected_features_ = []
        self.n_features_ = None

    def fit(self, X: np.ndarray, y: np.ndarray) -> None:
        X = np.asarray(X)
        n_samples, n_features = X.shape
        self.n_features_ = n_features

        included = []
        excluded = list(range(n_features))

        iterator = (
            trange(self.max_features, desc='SFS', leave=False)
            if self.verbose else range(self.max_features)
        )

        # Начальный baseline, если ни одного признака нет
        if included:
            X_base = X[:, included]
            base_scores = cross_val_score(
                clone(self.model),
                X_base,
                y,
                cv=self.cv,
                scoring="r2",
                n_jobs=-1,
            )
        else:
            # Если ни один признак не выбран, "фиктивный" baseline
            base_scores = np.zeros(len([-1 for _ in self.cv.split(X, y)]))

        for step in iterator:
            candidate_features = []
            candidate_scores = []
            candidate_pvals = []

            # Обновляем baseline на каждой итерации
            if included:
                X_base = X[:, included]
                base_scores = cross_val_score(
                    clone(self.model), X_base, y,
                    cv=self.cv, scoring="r2", n_jobs=-1
                )
            else:
                base_scores = np.zeros(len([-1 for _ in self.cv.split(X, y)]))

            for feat in excluded:
                selected = included + [feat]
                X_selected = X[:, selected]
                scores = cross_val_score(
                    clone(self.model), X_selected, y,
                    cv=self.cv, scoring="r2", n_jobs=-1
                )
                # Односторонний ttest_rel (alternative='greater')
                n_tests = len(excluded)
                if self.bonferroni:
                  alpha = self.alpha/n_tests
                else:
                  alpha = self.alpha

                stat, pval = ttest_rel(scores, base_scores, alternative='greater')
                if pval < self.alpha:
                    candidate_features.append(feat)
                    candidate_scores.append(scores.mean())
                    candidate_pvals.append(pval)

            if candidate_features:
                # Берем признак с максимальным средним приростом метрики
                best_idx = np.argmax(candidate_scores)
                best_feature = candidate_features[best_idx]
                included.append(best_feature)
                excluded.remove(best_feature)
                if self.verbose:
                    print(
                        f"Step {len(included)}/{self.max_features}: "
                        f"added feature {best_feature}, "
                        f"mean_score={candidate_scores[best_idx]:.4f}, "
                        f"pval={candidate_pvals[best_idx]:.4g}"
                    )
            else:
                # Нет ни одного значимого признака для добавления на этом шаге
                break

            if len(included) >= self.max_features:
                break

        self.selected_features_ = sorted(included)

    def transform(self, X: np.ndarray) -> np.ndarray:
        return X[:, self.selected_features_]

    @property
    def n_selected_features_(self):
        return len(self.selected_features_)

"""При множественном тестировании вероятность совершить ошибку I рода (отвергнуть нулевую гипотезу, когда она на самом деле верна) увеличивается с увеличением числа проверяемых гипотез. - **Поправка Бонферрони**"""

import numpy as np
from sklearn.base import clone
from sklearn.model_selection import cross_val_score
from scipy.stats import ttest_rel

try:
    from tqdm import trange
except ImportError:
    trange = range

class SequentialForwardSelector:
    """
    Sequential forward selection с t-test для отбора признаков и поддержкой Бонферрони.
    """

    def __init__(
        self,
        model,
        cv,
        max_features: int = 10,
        verbose: int = 0,
        alpha: float = 0.05,
        bonferroni: bool = True,
    ) -> None:
        self.model = model
        self.cv = cv
        self.max_features = max_features
        self.verbose = verbose
        self.alpha = alpha
        self.bonferroni = bonferroni
        self.selected_features_ = []
        self.n_features_ = None

    def fit(self, X: np.ndarray, y: np.ndarray) -> None:
        X = np.asarray(X)
        n_samples, n_features = X.shape
        self.n_features_ = n_features

        included = []
        excluded = list(range(n_features))

        iterator = (
            trange(self.max_features, desc='SFS', leave=False)
            if self.verbose else range(self.max_features)
        )

        for step in iterator:
            # Определяем alpha на этой итерации с учетом Бонферрони
            n_tests = len(excluded)
            if n_tests == 0:
                break

            if self.bonferroni:
                alpha = self.alpha / n_tests
            else:
                alpha = self.alpha

            # baseline (уже отобранные фичи)
            if included:
                X_base = X[:, included]
                base_scores = cross_val_score(
                    clone(self.model),
                    X_base,
                    y,
                    cv=self.cv,
                    scoring="r2",
                    n_jobs=-1,
                )
            else:
                base_scores = np.zeros(len([-1 for _ in self.cv.split(X, y)]))

            candidate_features = []
            candidate_scores = []
            candidate_pvals = []

            # Перебираем все ещё не выбранные фичи
            for feat in excluded:
                selected = included + [feat]
                X_selected = X[:, selected]
                scores = cross_val_score(
                    clone(self.model), X_selected, y,
                    cv=self.cv, scoring="r2", n_jobs=-1
                )
                stat, pval = ttest_rel(
                    scores, base_scores, alternative='greater'
                )
                # Сравниваем с alpha после Бонферрони!
                if pval < alpha:
                    candidate_features.append(feat)
                    candidate_scores.append(scores.mean())
                    candidate_pvals.append(pval)

            if candidate_features:
                best_idx = np.argmax(candidate_scores)
                best_feature = candidate_features[best_idx]
                included.append(best_feature)
                excluded.remove(best_feature)
                if self.verbose:
                    print(
                        f"Step {len(included)}/{self.max_features}: "
                        f"added feature {best_feature}, "
                        f"mean_score={candidate_scores[best_idx]:.4f}, "
                        f"pval={candidate_pvals[best_idx]:.4g}, "
                        f"alpha={alpha:.4g}"
                    )
            else:
                # Нет ни одного значимого признака для добавления
                break

            if len(included) >= self.max_features:
                break

        self.selected_features_ = sorted(included)

    def transform(self, X: np.ndarray) -> np.ndarray:
        return X[:, self.selected_features_]

    @property
    def n_selected_features_(self):
        return len(self.selected_features_)

"""**Линейная регрессия с L1 регуляризацией (Lasso)**"""

import numpy as np
from sklearn.linear_model import LassoCV
from typing import Tuple

import numpy as np
from sklearn.datasets import make_regression
from sklearn.model_selection import RepeatedKFold

class LassoSelector:
    """
    Lasso selector.
    Select features using Linear Regression with Lasso regularization.

    Parameters
    ----------
    cv: cross-validation generator :
        cross-validation
    alphas: List[float] :
        list of alphas for Lasso
    random_state: int :
        random state for reproducibility

    Attributes
    ----------
    n_features_: int :
        number of features
    selected_features_: List[int] :
        list of selected features
    n_selected_features_: int :
        number of selected features
    """

    def __init__(self, cv, alphas, random_state=42):
        self.cv = cv
        self.alphas = alphas
        self.random_state = random_state
        self.n_features_ = None
        self.selected_features_ = None
        # Не храним LassoCV, чтобы быть в духе селектора sklearn

    def fit(self, X: np.ndarray, y: np.ndarray) -> None:
        """
        Fit model.

        Parameters
        ----------
        X: np.ndarray :
            features
        y: np.ndarray :
            target

        Returns
        -------
        None
        """
        lasso = LassoCV(alphas=self.alphas, cv=self.cv, random_state=self.random_state)
        lasso.fit(X, y)
        # Выделяем индексы ненулевых коэффициентов
        mask = lasso.coef_ != 0
        self.n_features_ = X.shape[1]
        self.selected_features_ = np.where(mask)[0].tolist()
        return self

    def transform(self, X: np.ndarray) -> np.ndarray:
        """
        Reduce features to selected features.

        Parameters
        ----------
        X: np.ndarray :
            features

        Returns
        -------
        X: np.ndarray :
            reduced features

        """
        if self.selected_features_ is None:
            raise RuntimeError("The selector has not been fitted yet")
        return X[:, self.selected_features_]

    @property
    def n_selected_features_(self):
        return len(self.selected_features_) if self.selected_features_ is not None else 0

"""Solution for Kaggle AB2."""
def generate_dataset(
    n_samples: int = 10_000,
    n_features: int = 50,
    n_informative: int = 10,
    random_state: int = 42,
) -> Tuple:
    """
    Generate datasets.

    Parameters
    ----------
    n_samples: int :
        (Default value = 10_000)
        number of samples
    n_features: int :
        (Default value = 50)
        number of features
    n_informative: int :
        (Default value = 10)
        number of informative features, other features are noise
    random_state: int :
        (Default value = 42)
        random state for reproducibility

    Returns
    -------
    X: np.ndarray :
        features
    y: np.ndarray :
        target

    """
    X, y = make_regression(
        n_samples=n_samples,
        n_features=n_features,
        noise=100,
        random_state=random_state,
        n_informative=n_informative,
        bias=100,
        shuffle=True,
    )
    return X, y


def run() -> None:
    """Run."""
    random_state = 42
    n_samples = 10_000
    n_features = 50
    n_informative = 5
    n_splits = 3
    n_repeats = 10
    alphas = [2, 10]

    # generate data
    X, y = generate_dataset(n_samples, n_features, n_informative, random_state)

    # define model and cross-validation
    cv = RepeatedKFold(
        n_splits=n_splits, n_repeats=n_repeats, random_state=random_state
    )
    print(f"Baseline features count: {X.shape[1]}")

    # lasso selector
    selector = LassoSelector(cv, alphas, random_state)
    selector.fit(X, y)

    # show scores
    print(f"Features count: {selector.n_features_}")
    print(f"selected features: {selector.selected_features_}")
    print(f"Selected features count: {selector.n_selected_features_}")


if __name__ == "__main__":
    run()