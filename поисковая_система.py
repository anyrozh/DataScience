# -*- coding: utf-8 -*-
"""Поисковая_система.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1og-jXsOe_68VRdD8fQUcrDD4F07BWjla
"""

from typing import List
import pandas as pd

def get_documents(data_path: str) -> List[str]:
    dataset = pd.read_csv(data_path, names = ['description'])
    dataset = dataset.dropna().drop_duplicates()

    return dataset.description.tolist()

documents = get_documents('dataset.csv')

"""Самое простое действие, которое мы можем выполнить: после получения запроса пользователя, найти все документы, полностью соответствующие этому запросу."""

def search_dummy(documents: List[str], query: str, limit: int = 10) -> List[str]:
    result = []
    count = i = 0

    while count < limit and i < len(documents):
        if query in documents[i]:
            result.append(documents[i])
            count += 1

        i += 1

    return result

results = search_dummy(documents, 'smartphone', limit=2)
print('Top 2 search results for `smartphone`:')
print([doc[:100] for doc in results[:2]])

results = search_dummy(documents, 'SmartphonE', limit=2)
print('Top 2 search results for `SmartphonE`:')
print([doc[:100] for doc in results[:2]])

# >> Output:
# >> Top 2 search results for `smartphone`:
# >> Shinco 80 cm (32 Inches) HD Ready Smart LED TV SO32AS (Black) (2019 model) Size name:32 Inches   Thi
# >> Amazon Brand - Solimo 12-inch Wall Clock - Checkered (Silent Movement, Black Frame) Bring function a

# >> Top 2 search results for `SmartphonE`:
# >> []

"""**Очевидно, что необходимо сделать определенную предобработку запроса пользователя и описаний товаров**

Уберем все символы кроме букв и чисел, а так же типичные суффиксы:

['ation', 'ition', 'able', 'tion', 'ness', 'ious', 'ment', 'ible', 'ity', 'ial', 'ies', 'ing', 'ive', 'ion', 'er', 'al', 'ed', 'ly', 'ty', 'or', 'es']

Удаление суффиксов можно рассматривать как простой стемминг, то есть **выделение корней слов**. На практике для этого применяются более сложные методы, такие как стеммер Портера из пакета nltk.
"""

from typing import List
from tqdm import tqdm
import re


def preprocess_documents(documents: List[str], show_progress: bool = False) -> List[str]:
    """
    Preprocess a list of documents by applying several text normalization steps:
        1. Lowercasing all characters.
        2. Removing non-alphanumeric characters (excluding spaces).
        3. Stripping leading and trailing spaces.
        4. Removing extra spaces within the text.
        5. Stemming each word using a `stem` function.

    Parameters:
        documents (List[str]): A list of documents to be preprocessed.
        show_progress (bool, optional): A flag to show a progress bar. Default is False.

    Returns:
        List[str]: A list of preprocessed documents.
    """

    def stem(word: str) -> str:
        """
        Remove common suffixes from a given word.
        """
        suffixes = [
            'ation', 'ition', 'able', 'tion', 'ness', 'ious', 'ment', 'ible',
            'ity', 'ial', 'ies', 'ing', 'ive', 'ion', 'er', 'al', 'ed', 'ly', 'ty', 'or', 'es'
        ]

        for suf in suffixes:
          if word.endswith(suf):
            word = word[:-len(suf)]
            break
        return word

    processed_docs = []
    iterator = tqdm(documents, desc="Preprocessing documents") if show_progress else documents

    for doc in iterator:
        doc = doc.lower()
        doc = re.sub(r'[^a-z0-9\s]', '', doc)
        doc = ' '.join(doc.strip().split())
        words = doc.split()
        stemmed_words = [stem(word) for word in words]
        processed_doc = ' '.join(stemmed_words)
        processed_docs.append(processed_doc)

    return processed_docs

def search_dummy(prc_documents: List[str], query: str, limit: int = 10) -> List[int]:
    """
    Searches through a list of preprocessed documents to find documents
    containing the preprocessed query string.

    The query must be preprocessed using the same method as the documents.

    Parameters:
        prc_documents (List[str]): A list of preprocessed documents to search through.
        query (str): The query string to search for within the preprocessed documents.
        limit (int, optional): The maximum number of document indices to return.

    Returns:
        List[int]:
            A list of indices representing the positions
            in the `prc_documents` list where the preprocessed query appears.
    """
    """
    Выполняет поиск по списку предварительно обработанных документов для поиска нужных документов
    содержащий предварительно обработанную строку запроса.

    Запрос должен быть предварительно обработан тем же методом, что и документы.

    Параметры:
        prc_documents (List[str]): Список предварительно обработанных документов для поиска.
        запрос (str): строка запроса для поиска в предварительно обработанных документах.
        ограничение (int, необязательно): максимальное количество возвращаемых индексов документа.

    Возвращается:
        List[int]:
            Список индексов, представляющих позиции
            в списке "prc_documents", где отображается предварительно обработанный запрос.
    """
    query = query.lower()
    query = re.sub(r'[^a-z0-9\s]', '', query)
    query = ' '.join(query.strip().split())
    words = query.split()

    stemmed_words = []
    for word in words:
        for suffix in [
            'ation', 'ition', 'able', 'tion', 'ness', 'ious', 'ment', 'ible',
            'ity', 'ial', 'ies', 'ing', 'ive', 'ion', 'er', 'al', 'ed', 'ly', 'ty', 'or', 'es'
        ]:
            if word.endswith(suffix):
                word = word[:-len(suffix)]
                break
        stemmed_words.append(word)
    processed_query = ' '.join(stemmed_words)

    results = []
    count = 0

    for idx, doc in enumerate(prc_documents):
        if processed_query in doc:
            results.append(idx)
            count += 1
            if count >= limit:
                break

    return results

prc_documents = preprocess_documents(documents, True)
print('Top 2 preprocessed documents:')
print([doc[:100] for doc in prc_documents[:2]])

results = search_dummy(prc_documents, 'SmartphonE', limit=2)
print('Top 2 search results for `SmartphonE`:')
print([documents[doc_idx][:100] for doc_idx in results])

results = search_dummy(prc_documents, 'smrtaphone', limit=2)
print('Top 2 search results for `smrtaphone`:')
print([documents[doc_idx][:100] for doc_idx in results])

results = search_dummy(prc_documents, 'tab', limit=2)
print('Top 2 search results for `tab`:')
print([documents[doc_idx][:100] for doc_idx in results])

# >> Output:
# >> Top 2 preprocessed documents:
# >> 'pap plane design fram wall hang motivation office dec art prints 87 x 87 inch set of 4 paint made up',
# >> 'saf flor fram paint wood 30 inch x 10 inch spec effect uv print textur sao297 paint made up in synth'

# >> Top 2 search results for `SmartphonE`:
# >> 'Amazon Brand - Solimo 12-inch Wall Clock - Checkered (Silent Movement, Black Frame) Bring function a'
# >> 'Amazon Brand - Solimo 11-inch Wall Clock (Step Movement, Brown Frame) Style:Brown Frame   Bring func'

# >> Top 2 search results for `smrtaphone`:
# >> []

# >> Top 2 search results for `tab`:
# >> 'ART DIOR | Set of 3 Modern Art Prints - Winter Tree Printed on a Single Print | Canvas Wall Art | Un',
# >> 'ART DIOR | Dancing Village Girls | Canvas Wall Art | Unframed Canvas Art Print | 18 inch x 46 inch |'

"""1. **Устойчивость к опечаткам**
Если пользователь делает опечатку в запросе, N-граммы позволяют системе найти совпадения по частям слова. Например, слово "smrtaphone" можно разбить на **триграммы (3-граммы)** "smr", "mrt", "rta", "tap", "aph", "pho", "hon", "one". Большая их часть  пересекается с триграммами слова "smartphone" ("sma", "mar", "art", "rth", "tho", "hon", "one"), что позволяет системе понять, что запрос, вероятно, имел в виду "smartphone".
2. **Поиск по частичным запросам.** Пользователи часто вводят неполные слова, например, "smar" вместо "smartphone". N-граммы разбивают слова на более короткие последовательности, что позволяет находить совпадения по частям слова. В этом случае триграммы "sma", "mar" из запроса "smar" найдут совпадения в слове "smartphone".
3. **Улучшение релевантности**
N-граммы учитывают все возможные сочетания символов в запросе, что повышает релевантность результатов поиска. Это особенно важно в больших текстовых массивах, где точное совпадение может быть редким.
"""

from typing import List
import pandas as pd

def get_documents(data_path: str) -> List[str]:
    dataset = pd.read_csv(data_path, names = ['description'])
    dataset = dataset.dropna().drop_duplicates()

    return dataset.description.tolist()

documents = get_documents('dataset.csv')

from typing import List
from tqdm import tqdm
from nltk.util import ngrams
import re


def preprocess_documents(documents: List[str], show_progress: bool = False) -> List[str]:
    """
    Preprocess a list of documents by applying several text normalization steps:
        1. Lowercasing all characters.
        2. Removing non-alphanumeric characters (excluding spaces).
        3. Stripping leading and trailing spaces.
        4. Removing extra spaces within the text.
        5. Stemming each word using a `stem` function.
    """
    def stem(word: str) -> str:
        suffixes = [
            'ation', 'ition', 'able', 'tion', 'ness', 'ious', 'ment', 'ible',
            'ity', 'ial', 'ies', 'ing', 'ive', 'ion', 'er', 'al', 'ed', 'ly', 'ty', 'or', 'es'
        ]
        for suffix in suffixes:
            if word.endswith(suffix):
                return word[:-len(suffix)]
        return word

    processed_docs = []
    iterator = tqdm(documents, desc="Preprocessing documents") if show_progress else documents

    for doc in iterator:
        doc = doc.lower()
        doc = re.sub(r'[^a-z0-9\s]', '', doc)
        doc = ' '.join(doc.strip().split())
        words = doc.split()
        stemmed_words = [stem(word) for word in words]
        processed_docs.append(' '.join(stemmed_words))

    return processed_docs

def documents_to_ngrams(documents: List[str], n_gram_size: int = 3, show_progress: bool = False) -> List[List[str]]:
    """
    Convert a list of documents into a list of lists of n-grams for each document.
    This function uses `preprocess_documents` to preprocess the input documents.

    Args:
        documents (List[str]): A list of strings, where each string is a document.
        n_gram_size (int): The size of the n-grams to generate. Default is 3.
        show_progress (bool): If True, displays a progress bar. Default is False.

    Returns:
        List[List[str]]: A list of lists, where each inner list contains n-grams of the corresponding document.
    """
    # Preprocess documents first
    #Если в doc_ngrams есть что-то, то возьми длину первого символа первой n-граммы.
    processed_docs = preprocess_documents(documents, show_progress)

    ngrams_list = []
    iterator = tqdm(processed_docs, desc="Generating n-grams") if show_progress else processed_docs

    for doc in iterator:
        doc_ngrams = []
        words = doc.split()
        for word in words:
            # Only generate n-grams for words that are at least n_gram_size long
            if len(word) >= n_gram_size:
                word_ngrams = [''.join(gram) for gram in ngrams(word, n_gram_size)]
                doc_ngrams.extend(word_ngrams)
            # Skip words shorter than n_gram_size
        ngrams_list.append(doc_ngrams)

    return ngrams_list

def search_ngrams(doc_ngrams: List[List[str]], query: str, limit: int = 10) -> List[int]:
    """
    Search for documents that contain n-grams matching those in the query.
    """
    # Preprocess the query
    query = query.lower()
    query = re.sub(r'[^a-z0-9\s]', '', query)
    query = ' '.join(query.strip().split())

    # Generate n-grams for the query (using same n_gram_size as documents)
    n_gram_size = len(doc_ngrams[0][0]) if doc_ngrams and doc_ngrams[0] else 3  # Default to 3 if empty
    query_words = query.split()
    query_ngrams = []

    for word in query_words:
        if len(word) >= n_gram_size:
            word_ngrams = [''.join(gram) for gram in ngrams(word, n_gram_size)]
            query_ngrams.extend(word_ngrams)
        else:
            query_ngrams.append(word)

    # Find documents with matching n-grams
    matching_docs = []


    for doc_idx, doc_grams in enumerate(doc_ngrams):
        # Count how many query n-grams match in this document
        matches = len(set(query_ngrams) & set(doc_grams))
        if matches > 0:
            matching_docs.append((doc_idx, matches))

    """
    - Пусть у тебя 5 документов.
    - Для запроса "hello" n-граммы найдены и по ним посчитано:
          - документ 1: 2 совпадения
          - документ 3: 5 совпадений
          - документ 0: 1 совпадение
    - После сортировки будет: [(3, 5), (1, 2), (0, 1)]
    - Если `limit=2`, то вернётся: [3, 1]
    """

    # Sort by number of matches (descending) and return indices
    matching_docs.sort(key=lambda x: x[1], reverse=True)
    result_indices = [doc_idx for doc_idx, _ in matching_docs[:limit]]

    return result_indices

doc_ngrams = documents_to_ngrams(documents)

print('Top 2 documents ngrams:')
for doc in doc_ngrams[:2]:
    print(doc[:10])

results = search_ngrams(doc_ngrams, 'smrtaphone', limit=2)
print('Top 2 search results for `smrtaphone`:')
print([documents[doc_idx][:100] for doc_idx in results])

# >> Output:
# >> ['pap', 'pla', 'lan', 'ane', 'des', 'esi', 'sig', 'ign', 'fra', 'ram']
# >> ['saf', 'flo', 'lor', 'fra', 'ram', 'pai', 'ain', 'int', 'woo', 'ood']

# >> Top 2 search results for `smrtaphone`:
# >> "Paper Plane Design Framed Wall Hanging Motivational Office Decor Art Prints (8.7 X 8.7 inch) - Set o\"
# >> "SAF 'UV Textured Modern Art Print Framed' Painting (Synthetic, 35 cm x 50 cm x 3 cm, Set of 3) Color"

"""Таким образом мы приходим к функции ранжирования, которая для каждой пары "запрос-документ" присваивает числовой показатель. Этот показатель увеличивается с ростом релевантности документа по отношению к запросу пользователя. В нашем случае в качестве такой функции мы будем использовать частоту совпадений **N-грамм или term frequency.**

Для этого при поиске по N-граммам мы будем определять не факт наличия совпадающих N-грамм, а подсчитывать общее количество совпадающих **N-грамм для каждой пары "запрос-документ"**. Получившееся число и будет **нашей мерой релевантности term frequency.**
"""

from typing import List, Dict, Tuple
from collections import Counter
from nltk.util import ngrams
from tqdm import tqdm
import re

# Используем вашу функцию препроцессинга; вы можете сделать её методом класса,
# но здесь для совместимости подключаем извне.
def preprocess_documents(documents: List[str], show_progress: bool = False) -> List[str]:
    def stem(word: str) -> str:
        suffixes = [
            'ation', 'ition', 'able', 'tion', 'ness', 'ious', 'ment', 'ible',
            'ity', 'ial', 'ies', 'ing', 'ive', 'ion', 'er', 'al', 'ed', 'ly', 'ty', 'or', 'es'
        ]
        for suffix in suffixes:
            if word.endswith(suffix):
                return word[:-len(suffix)]
        return word
    processed_docs = []
    iterator = tqdm(documents, desc="Preprocessing documents") if show_progress else documents
    for doc in iterator:
        doc = doc.lower()
        doc = re.sub(r'[^a-z0-9\s]', '', doc)
        doc = ' '.join(doc.strip().split())
        words = doc.split()
        stemmed_words = [stem(word) for word in words]
        processed_docs.append(' '.join(stemmed_words))
    return processed_docs

class TF:
    """Implementation of the TF ranking function."""
    def __init__(self, n_gram_size: int = 3, show_progress: bool = False):
        """Initialize the TF model."""
        self.documents = None
        self.n_gram_size = n_gram_size
        self.show_progress = show_progress

    def fit(self, documents: List[str]):
        """Fit the TF model to a list of documents."""
        self.documents = documents
        self.documents_ngrams = self._documents_to_ngrams(documents, self.n_gram_size)
        self.tf_index = self._calc_tf_index()

    def _preprocess_documents(self, documents: List[str]) -> List[str]:
        """Preprocess a list of documents."""
        return preprocess_documents(documents, show_progress=self.show_progress)

    def _documents_to_ngrams(self, documents: List[str], n_gram_size: int = 3) -> List[List[str]]:
        """
        Convert a list of documents into a list of lists of n-grams for each document.
        """
        processed_docs = self._preprocess_documents(documents)
        ngrams_list = []
        iterator = tqdm(processed_docs, desc="Generating n-grams") if self.show_progress else processed_docs
        for doc in iterator:
            doc_ngrams = []
            words = doc.split()
            for word in words:
                # Только для слов достаточной длины
                if len(word) >= n_gram_size:
                    word_ngrams = [''.join(gram) for gram in ngrams(word, n_gram_size)]
                    doc_ngrams.extend(word_ngrams)
            ngrams_list.append(doc_ngrams)
        return ngrams_list

    def _calc_tf_index(self) -> List[Dict]:
        """
        Create a term frequency index for a list of documents represented as lists of n-grams.
        """
        tf_index = []
        for doc_ngrams in self.documents_ngrams:
            tf_dict = dict(Counter(doc_ngrams))
            tf_index.append(tf_dict)
        return tf_index

    def search(self, query: str, limit: int = 10, show_results: bool = False, char_limit: int = 100) -> List[Tuple[int, float]]:
        """
        Search in a term frequency index for documents matching a query
        based on n-gram similarity (sum of matching n-gram counts in the index for scoring).
        """
        query_ngrams = self._documents_to_ngrams([query], self.n_gram_size)[0]
        if not query_ngrams:
            return []
        # Посчитаем частоту n-грамм запроса
        query_ngrams_counter = Counter(query_ngrams)
        scores = []
        for doc_idx, doc_tf in enumerate(self.tf_index):# Для каждой n-граммы запроса берем счетчик в документе и
            # суммируем с учетом числа повторов (query_ngrams_counter[ng])
            tf_score = 0
            for ng in query_ngrams_counter:
                count_in_doc = doc_tf.get(ng, 0)
                tf_score += count_in_doc * query_ngrams_counter[ng]
            if tf_score > 0:
                scores.append((doc_idx, float(tf_score)))
        # Сортируем по релевантности
        scores.sort(key=lambda x: x[1], reverse=True)
        if show_results:
            print(f"Top {limit} search results for {query}:")
            for doc_idx, score in scores[:limit]:
                preview = self.documents[doc_idx][:char_limit].replace('\n',' ')
                print(f"{score:.2f}: {preview}")
        return scores[:limit]

tf_engine = TF(show_progress=True)
tf_engine.fit(documents)
tf_engine.search('smrtaphone', limit=4, show_results=True)

# >> Output:
# >> Top 4 search results for `smrtaphone`:
# >> 152: Risk Savvy: How to Make Good Decisions About the Author GERD GIGERENZER is director of the Max Planc
# >> 152: Risk Savvy: How to Make Good Decisions About the Author Gerd Gigerenzer is the author of Gut Feeling
# >> 86: HP B4B09PA Headphones with Mic Product Description HP Headphones Overview With HP B4B09PA Over ear H
# >> 82: Mastering the Art of Selling Real Estate: Fully Revised and Updated About the Author Tom Hopkins is

tf_engine = TF(show_progress=True)

dummy_documents1 = [
    'smartphone',
    'frying pan',
]

tf_engine.fit(dummy_documents1)
tf_engine.search('smratphone', show_results=True)

dummy_documents2 = [
    'smartphone',
    'frying pan',
    'headphones for your smartphone',
]

tf_engine.fit(dummy_documents2)
tf_engine.search('smratphone', show_results=True)

# >> Output:
# >> Top 10 search results for `smratphone`:
# >> 4.00: smartphone
# >> 0.00: frying pan

# >> Top 10 search results for `smratphone`:
# >> 6.00: headphones for your smartphone
# >> 4.00: smartphone
# >> 0.00: frying pan

"""Чтобы сделать результаты поиска более точными, нужно учитывать относительную частоту N-грамм. Например, если N-грамма "sma" встречается в коротком слове "smart", она будет более значимой, чем если бы она встречалась в длинном слове "smartphone". Это можно сделать, нормализуя вклад каждой N-граммы по отношению к длине документа. **В результате, документы, где нужные N-граммы более концентрированы, будут оценены выше тех, где они встречаются реже или случайно.**"""

from typing import List, Dict, Tuple
from collections import Counter
from tqdm import tqdm
import re
from nltk.util import ngrams

class TF:
    """Implementation of the TF ranking function."""
    def __init__(self, n_gram_size: int = 3, show_progress: bool = False):
        """Initialize the TF model."""
        self.documents = None
        self.n_gram_size = n_gram_size
        self.show_progress = show_progress

    def fit(self, documents: List[str]):
        """Fit the TF model to a list of documents."""
        self.documents = documents
        self.documents_ngrams = self._documents_to_ngrams(documents, self.n_gram_size)
        self.tf_index = self._calc_tf_index()

    def _preprocess_documents(self, documents: List[str]) -> List[str]:
        """Preprocess a list of documents."""
        return preprocess_documents(documents, show_progress=self.show_progress)

    def _documents_to_ngrams(self, documents: List[str], n_gram_size: int = 3) -> List[List[str]]:
        """
        Convert a list of documents into a list of lists of n-grams for each document.
        """
        processed_docs = self._preprocess_documents(documents)
        ngrams_list = []
        iterator = tqdm(processed_docs, desc="Generating n-grams") if self.show_progress else processed_docs
        for doc in iterator:
            doc_ngrams = []
            words = doc.split()
            for word in words:
                # Только для слов достаточной длины
                if len(word) >= n_gram_size:
                    word_ngrams = [''.join(gram) for gram in ngrams(word, n_gram_size)]
                    doc_ngrams.extend(word_ngrams)
            ngrams_list.append(doc_ngrams)
        return ngrams_list

    def _calc_tf_index(self) -> List[Dict]:
        """
        Create a term frequency index for a list of documents represented as lists of n-grams.
        """
        tf_index = []
        for doc_ngrams in self.documents_ngrams:
            tf_dict = dict(Counter(doc_ngrams))
            tf_index.append(tf_dict)
        return tf_index

    def search(self, query: str, limit: int = 10, show_results: bool = False, char_limit: int = 100) -> List[Tuple[int, float]]:
        """
        Search in a term frequency index for documents matching a query
        based on n-gram similarity (sum of matching n-gram counts in the index for scoring).

        This function preprocesses the query and converts it into n-grams using the `_documents_to_ngrams` function.

        The scores are normalized by dividing the n-gram match score by the total number of n-grams in the document.

        Args:
            tf_index (List[Dict]): The term frequency index, where each dictionary contains n-gram counts for a document.
            query (str): The query string to search for.
            n_gram_size (int): The size of the n-grams to generate from the query. Default is 3.
            limit (int): The maximum number of results to return. Default is 10.

        Returns:
            List[Tuple[int, float]]: A sorted list of tuples (document index, score), ordered by relevance in descending order.
        """


    def search(self, query: str, limit: int = 10, show_results: bool = False, char_limit: int = 100) -> List[Tuple[int, float]]:
        """
        Search in a term frequency index for documents matching a query
        based on n-gram similarity (sum of matching n-gram counts in the index for scoring).
        """
        query_ngrams = self._documents_to_ngrams([query], self.n_gram_size)[0]
        if not query_ngrams:
            return []
        # Посчитаем частоту n-грамм запроса
        query_ngrams_counter = Counter(query_ngrams)
        scores = []
        for doc_idx, doc_tf in enumerate(self.tf_index):# Для каждой n-граммы запроса берем счетчик в документе и
            # суммируем с учетом числа повторов (query_ngrams_counter[ng])
            tf_score = 0
            for ng in query_ngrams_counter:
                count_in_doc = doc_tf.get(ng, 0)
                tf_score += (count_in_doc * query_ngrams_counter[ng])
            if tf_score > 0:
                scores.append((doc_idx, float(tf_score)))
        # Сортируем по релевантности
        scores.sort(key=lambda x: x[1], reverse=True)
        if show_results:
            print(f"Top {limit} search results for {query}:")
            for doc_idx, score in scores[:limit]:
                preview = self.documents[doc_idx][:char_limit].replace('\n',' ')
                print(f"{score:.2f}: {preview}")
        return scores[:limit]

from typing import List, Dict, Tuple
from collections import Counter
from nltk.util import ngrams
from tqdm import tqdm
import re

def preprocess_documents(documents: List[str], show_progress: bool = False) -> List[str]:
    def stem(word: str) -> str:
        suffixes = [
            'ation', 'ition', 'able', 'tion', 'ness', 'ious', 'ment', 'ible',
            'ity', 'ial', 'ies', 'ing', 'ive', 'ion', 'er', 'al', 'ed', 'ly', 'ty', 'or', 'es'
        ]
        for suffix in suffixes:
            if word.endswith(suffix):
                return word[:-len(suffix)]
        return word
    processed_docs = []
    iterator = tqdm(documents, desc="Preprocessing documents") if show_progress else documents
    for doc in iterator:
        doc = doc.lower()
        doc = re.sub(r'[^a-z0-9\s]', '', doc)
        doc = ' '.join(doc.strip().split())
        words = doc.split()
        stemmed_words = [stem(word) for word in words]
        processed_docs.append(' '.join(stemmed_words))
    return processed_docs

class TF:
    """Implementation of the TF ranking function."""

    def __init__(self, n_gram_size: int = 3, show_progress: bool = False):
        """Initialize the TF model."""
        self.documents = None
        self.n_gram_size = n_gram_size
        self.show_progress = show_progress

    def fit(self, documents: List[str]):
        """Fit the TF model to a list of documents."""
        self.documents = documents
        self.documents_ngrams = self._documents_to_ngrams(documents, self.n_gram_size)
        self.tf_index = self._calc_tf_index()

    def _preprocess_documents(self, documents: List[str]) -> List[str]:
        """Preprocess a list of documents."""
        return preprocess_documents(documents, show_progress=self.show_progress)

    def _documents_to_ngrams(self, documents: List[str], n_gram_size: int = 3) -> List[List[str]]:
        """
        Convert a list of documents into a list of lists of n-grams for each document.
        """
        processed_docs = self._preprocess_documents(documents)
        ngrams_list = []
        iterator = tqdm(processed_docs, desc="Generating n-grams") if self.show_progress else processed_docs
        for doc in iterator:
            doc_ngrams = []
            words = doc.split()
            for word in words:
                if len(word) >= n_gram_size:
                    word_ngrams = [''.join(gram) for gram in ngrams(word, n_gram_size)]
                    doc_ngrams.extend(word_ngrams)
            ngrams_list.append(doc_ngrams)
        return ngrams_list

    def _calc_tf_index(self) -> List[Dict[str, int]]:
        """
        Create a term frequency index for a list of documents represented as lists of n-grams.
        """
        tf_index = []
        for doc_ngrams in self.documents_ngrams:
            tf_dict = dict(Counter(doc_ngrams))
            tf_index.append(tf_dict)
        return tf_index

    def search(
        self,
        query: str,
        limit: int = 10,
        show_results: bool = False,
        char_limit: int = 100
    ) -> List[Tuple[int, float]]:
        """
        Search in a term frequency index for documents matching a query
        based on n-gram similarity (sum of matching n-gram counts in the index for scoring),
        with normalization by document n-gram length.
        """
        query_ngrams = self._documents_to_ngrams([query], self.n_gram_size)[0]
        if not query_ngrams:
            return []
        query_ngrams_counter = Counter(query_ngrams)
        scores = []

        for doc_idx, doc_tf in enumerate(self.tf_index):
            tf_score = 0
            # Считаем совпадения n-грамм между запросом и документом
            for ng in query_ngrams_counter:
                count_in_doc = doc_tf.get(ng, 0)
                tf_score += count_in_doc * query_ngrams_counter[ng]
            # Считаем общее число n-грамм в документе
            total_doc_ngrams = sum(doc_tf.values())
            if tf_score > 0 and total_doc_ngrams > 0:
                norm_score = tf_score / total_doc_ngrams
                scores.append((doc_idx, norm_score))
        # Сортировка по убыванию релевантности
        scores.sort(key=lambda x: x[1], reverse=True)
        if show_results:
            print(f"Top {limit} search results for {query}:")
            for doc_idx, score in scores[:limit]:
                preview = self.documents[doc_idx][:char_limit].replace('\n', ' ')
                print(f"{score:.2f}: {preview}")
        return scores[:limit]

dummy_documents = [
    'smartphone',
    'frying pan',
    'headphones for your smartphone',
]

tf_engine = TF(show_progress=True)
tf_engine.fit(dummy_documents)
tf_engine.search('smratphone', limit=4, show_results=True)
# 0.5: smartphone
# 0.38: headphones for your smartphone
# 0.0: frying pan

"""**IDF: TF-IDF**

**IDF — это мера того, насколько редко встречается N-грамма среди всех документов.** Интуитивно, чем р**еже встречается N-грамма, тем больше информации она несет**. Например, если слово "the" встречается практически в каждом документе, то оно не дает никакой полезной информации для поиска, так как не помогает выделить конкретный документ. С другой стороны, если триграмма "sma" встречается только в одном документе, то это позволяет нам предположить, что именно этот документ наиболее релевантен для нашего запроса.
"""

from typing import List, Dict, Tuple
from collections import defaultdict, Counter
import numpy as np
import re
from tqdm import tqdm
from nltk.util import ngrams


class TFIDF:
    """Implementation of the TF-IDF ranking function."""
    def __init__(self, n_gram_size: int = 3, show_progress: bool = False):
        """Initialize the TF-IDF model."""
        self.documents = None
        self.n_gram_size = n_gram_size
        self.show_progress = show_progress

    def fit(self, documents: List[str]):
        """Fit the TF-IDF model to a list of documents."""
        self.documents = documents
        self.documents_ngrams = self._documents_to_ngrams(documents)

        self.tf_index = self._calc_tf_index()
        self.idf_index = self._calc_idf_index()

    def _preprocess_documents(self, documents: List[str], progress: bool = False) -> List[str]:
        """Preprocess a list of documents."""
        def stem(word: str) -> str:
            """Remove common suffixes from a given word."""
            suffixes = [
                'ation', 'ition', 'able', 'tion', 'ness', 'ious', 'ment', 'ible',
                'ity', 'ial', 'ies', 'ing', 'ive', 'ion', 'er', 'al', 'ed', 'ly', 'ty', 'or', 'es'
            ]

            for suffix in suffixes:
                if word.endswith(suffix):
                    return word[:-len(suffix)]

            return word

        iterator = tqdm(documents, desc="Preprocessing documents") if progress else documents

        prc_documents = []

        for document in iterator:
            document = re.sub(r'[^a-z0-9\s]', '', document.lower())
            document = re.sub(r'\s+', ' ', document).strip()
            stemmed_words = [stem(word) for word in document.split()]
            prc_document = ' '.join(word for word in stemmed_words if word)
            prc_documents.append(prc_document)

        return prc_documents

    def _documents_to_ngrams(self, documents: List[str]) -> List[List[str]]:
        """
        Convert a list of documents into a list of lists of n-grams for each document.
        This function uses `_preprocess_documents` to preprocess the input documents.

        Args:
            documents (List[str]): A list of strings, where each string is a document.

        Returns:
            List[List[str]]: A list of lists, where each inner list contains n-grams of the corresponding document.
        """
        processed_docs = self._preprocess_documents(documents, self.show_progress)
        ngrams_list = []
        iterator = tqdm(processed_docs, desc="Generating n-grams") if self.show_progress else processed_docs

        for doc in iterator:
            doc_ngrams = []
            words = doc.split()
            for word in words:
                if len(word) >= self.n_gram_size:
                    word_ngrams = [''.join(gram) for gram in ngrams(word, self.n_gram_size)]
                    doc_ngrams.extend(word_ngrams)
            ngrams_list.append(doc_ngrams)
        return ngrams_list

    def _calc_idf_index(self) -> Dict[str, float]:
        """
        Calculate the Inverse Document Frequency (IDF) for n-grams in a collection of documents.

        This implementation uses the natural logarithm (ln) for IDF calculation and
        adds 1 to both the numerator and the denominator to avoid division by zero
        and to smooth the IDF values.

        Returns:
            Dict[str, float] A dictionary where the keys are n-grams and the values are their corresponding IDF scores.
        """
        N = len(self.documents_ngrams)
        df = defaultdict(int)

        for doc_ngrams in self.documents_ngrams:
            for ng in set(doc_ngrams):
                df[ng] += 1

        idf_index = {}
        for ng, df_t in df.items():
            idf_index[ng] = np.log((N + 1) / (df_t + 1))

        return idf_index

    def _calc_tf_index(self) -> List[Dict]:
        """
        Create a term frequency index for a list of documents represented as lists of n-grams.

        Returns:
            List[Dict]: A list of dictionaries, where each dictionary represents the term frequency (TF)
                count of n-grams in the corresponding document.
        """
        tf_index = []
        for doc_ngrams in self.documents_ngrams:
            tf_dict = Counter(doc_ngrams)
            total_ngrams = sum(tf_dict.values())
            # Normalized TF (term frequency divided by total terms in document)
            normalized_tf = {k: v / total_ngrams for k, v in tf_dict.items()}
            tf_index.append(normalized_tf)
        return tf_index

    def search(self, query: str, limit: int = 10, show_results: bool = False, char_limit: int = 100) -> List[Tuple[int, float]]:
        """
        Search for documents similar to a query using TF-IDF scores.

        Args:
            query (str): The query string to search for.
            limit (int): Maximum number of results to return (default is 10).
            show_results (bool): Whether to print the top search results (default is False).
            char_limit (int): Maximum number of characters to display for each search result (default is 100).

        Returns:
            List[Tuple[int, float]]: A list of tuples containing document indices and their corresponding match scores,
                sorted by relevance.
        """
        query_ngrams = self._documents_to_ngrams([query])[0]
        if not query_ngrams:
            return []

        query_ngrams_counter = Counter(query_ngrams)
        scores = []

        for doc_idx, doc_tf in enumerate(self.tf_index):
            tfidf_score = 0.0
            for ng in query_ngrams_counter:
                if ng in self.idf_index:
                    tf = doc_tf.get(ng, 0.0)
                    idf = self.idf_index[ng]
                    tfidf_score += tf * idf * query_ngrams_counter[ng]

            if tfidf_score > 0:
                scores.append((doc_idx, tfidf_score))

        scores.sort(key=lambda x: x[1], reverse=True)

        if show_results:
            print(f"Top {limit} search results for `{query}`:")
            for rank, (doc_idx, score) in enumerate(scores[:limit], 1):
                preview = self.documents[doc_idx][:char_limit].replace('\n', ' ')
                print(f"{rank}. {score:.2f}: {preview}")

        return scores[:limit]

from typing import List, Tuple, Dict
from collections import Counter, defaultdict
import re
import numpy as np
from tqdm import tqdm


class BM25:
    """Implementation of the BM25 ranking function."""
    def __init__(self, k1=1.5, b=0.75, show_progress: bool = False):
        """Initialize the BM25 instance with the given parameters."""
        self.documents = None
        self.k1 = k1
        self.b = b
        self.show_progress = show_progress

    def fit(self, documents: List[str]):
        """Fit the BM25 model on the given documents."""
        self.documents = documents
        self.documents_words = self._documents_to_words(documents)
        self.total_documents = len(self.documents)
        self.avg_doc_length = np.mean([len(doc) for doc in self.documents_words])

        self.tf_index = self._calc_tf_index()
        self.idf_index = self._calc_idf_index()

    def _preprocess_documents(self, documents: List[str], progress: bool = False) -> List[str]:
        """Preprocess a list of documents."""
        def stem(word: str) -> str:
            """Remove common suffixes from a given word."""
            suffixes = [
                'ation', 'ition', 'able', 'tion', 'ness', 'ious', 'ment', 'ible',
                'ity', 'ial', 'ies', 'ing', 'ive', 'ion', 'er', 'al', 'ed', 'ly', 'ty', 'or', 'es'
            ]

            for suffix in suffixes:
                if word.endswith(suffix):
                    return word[:-len(suffix)]

            return word

        iterator = tqdm(documents, desc="Preprocessing documents") if progress else documents

        prc_documents = []

        for document in iterator:
            document = re.sub(r'[^a-z0-9\s]', '', document.lower())
            document = re.sub(r'\s+', ' ', document).strip()
            stemmed_words = [stem(word) for word in document.split()]
            prc_document = ' '.join(word for word in stemmed_words if word)
            prc_documents.append(prc_document)

        return prc_documents

    def _documents_to_words(self, documents: List[str]) -> List[List[str]]:
        """Preprocess and tokenize documents into words."""
        prc_documents = self._preprocess_documents(documents, self.show_progress)
        documents_words = [doc.split(' ') for doc in prc_documents]

        return documents_words

    def _calc_tf_index(self) -> List[Dict]:
        """
        Calculate the term frequency (TF) index for the documents.
        Each document is represented as a dictionary where the keys are words
        and the values are the frequencies of those words in the document.

        Returns:
            List[Dict]: List of dictionaries with term frequencies for each document.
        """
        result = []

        for doc in self.documents_words:
            result.append(dict(Counter(doc)))

        return result



    def _tf(self, word_tf: int, doc_length: int) -> float:
        """
        Calculate the term frequency (TF) score for a word using the BM25 formula.

        Args:
            word_tf (int): Term frequency of the word in the document.
            doc_length (int): Length of the document.

        Returns:
            float: TF score.
        """
        numerator = word_tf * (self.k1 + 1)
        denominator = word_tf + self.k1 * (1 - self.b + self.b * (doc_length / self.avg_doc_length))
        return numerator / denominator


    def _calc_idf_index(self) -> Dict[str, float]:
        """
        Calculate the inverse document frequency (IDF) index for the documents using the BM25 formula.

        Returns:
            Dict[str, float]: Dictionary with IDF scores for each word.
        """
        idf_index = {}
        N = self.total_documents

        # First calculate document frequencies for each term
        doc_freq = defaultdict(int)
        for doc_tf in self.tf_index:
            for word in doc_tf:
                doc_freq[word] += 1

        # Then calculate IDF for each term
        for word, df in doc_freq.items():
            numerator = N - df + 0.5
            denominator = df + 0.5
            idf = np.log(numerator / denominator + 1)  # Adding 1 to prevent negative values
            idf_index[word] = idf

        return idf_index

    def search(self, query: str, limit: int = 10, show_results: bool = False, char_limit: int = 100) -> List[Tuple[int, float]]:
        """
        Search for documents similar to a query using BM25 scores.

        Args:
            query (str): Query string to search for.
            limit (int): Number of top results to return. Default is 10.
            char_limit (int): Character limit for displaying document snippets. Default is 100.

        Returns:
            List[Tuple[int, float]]: List of tuples with document indices and their scores.

        Raises:
            ValueError: If the model has not been fitted with documents.
        """
        if self.documents is None:
            raise ValueError("Model has not been fitted with documents. Call fit() first.")

        # Preprocess the query
        query_words = self._documents_to_words([query])[0]
        if not query_words:
            return []

        scores = []

        for doc_idx, doc_tf in enumerate(self.tf_index):
            doc_length = len(self.documents_words[doc_idx])
            score = 0.0

            for word in query_words:
                if word in doc_tf:
                    tf_score = self._tf(doc_tf[word], doc_length)
                    idf_score = self.idf_index.get(word, 0)
                    score += tf_score * idf_score

            if score > 0:
                scores.append((doc_idx, score))

        # Sort by score in descending order
        scores.sort(key=lambda x: x[1], reverse=True)

        if show_results:
            print(f"Top {limit} search results for `{query}`:")
            for rank, (doc_idx, score) in enumerate(scores[:limit], 1):
                preview = self.documents[doc_idx][:char_limit].replace('\n', ' ')
                print(f"{score:.2f}: {preview}")

        return scores[:limit]

engine = BM25(show_progress=True)
engine.fit('/content/documents.json')
engine.search('bicycle accessories', limit=4, show_results=True)
engine.search('Living Room Plants', limit=4, show_results=True)
engine.search('iph', limit=4, show_results=True)
engine.search('eyephone', limit=4, show_results=True)

# >> Output:
# >> Top 4 search results for `bicycle accessories`:
# >> 13.75: Bulfyss Imported Multi-Purpose Mountain Bike Bicycle Repair Tool Kit Allen Key Spoke Wrench This mul
# >> 12.04: Cartshopper High Power 18650 Headlamp 1800LM CREE XM-L T6 LED Hunting Bicycle Camping Head Torch Lig
# >> 11.38: HOKIPO® Premium Steel Bicycle Pizza Cutter Slicer Knife Tool (Red) Slice & serve up some fun with th
# >> 10.61: ONESPORT Women's Polyester Black Shorts(ONSP11BL-P) Womens bicycle shorts in stretch polyester jerse

# >> Top 4 search results for `Living Room Plants`:
# >> 17.71: Ashiyanadecors Artificial Flower Basket Carnation Yellow Natural Looking Perfect decorative plants f
# >> 15.74: First Smart Deal Plastic Round Pot (12-inch, Brown, Pack of 5) UV stabilized round plastic planter t
# >> 15.74: First Smart Deal Plastic Rectangle Pot Set (Brown, Pack of 3) UV stabilized rectangle plastic plante
# >> 15.74: First Smart Deal Plastic Round Pot (10-inch, Brown, Pack of 4) UV stabilized round plastic planter t

# >> Top 4 search results for `iph`:
# >> 0.00: Paper Plane Design Framed Wall Hanging Motivational Office Decor Art Prints (8.7 X 8.7 inch) - Set o
# >> 0.00: SAF 'Floral' Framed Painting (Wood, 30 inch x 10 inch, Special Effect UV Print Textured, SAO297) Pai
# >> 0.00: SAF 'UV Textured Modern Art Print Framed' Painting (Synthetic, 35 cm x 50 cm x 3 cm, Set of 3) Color
# >> 0.00: SAF Flower Print Framed Painting (Synthetic, 13.5 inch x 22 inch, UV Textured, Set of 3, SANFSW4951)

# >> Top 4 search results for `eyephone`:
# >> 0.00: Paper Plane Design Framed Wall Hanging Motivational Office Decor Art Prints (8.7 X 8.7 inch) - Set o
# >> 0.00: SAF 'Floral' Framed Painting (Wood, 30 inch x 10 inch, Special Effect UV Print Textured, SAO297) Pai
# >> 0.00: SAF 'UV Textured Modern Art Print Framed' Painting (Synthetic, 35 cm x 50 cm x 3 cm, Set of 3) Color
# >> 0.00: SAF Flower Print Framed Painting (Synthetic, 13.5 inch x 22 inch, UV Textured, Set of 3, SANFSW4951)

engine = TFIDF(n_gram_size=3, show_progress=True)
engine.fit('/content/documents.json')
engine.search('the heavy simple beautiful black iphone', show_results=True)

# >> Output:
# >> Top 4 search results for `the heavy simple beautiful black iphone`:
# >> 1.76: Black Beauty
# >> 1.20: Apple iPhone 7 (Jet Black, 32GB)
# >> 0.69: POPIO Tempered Glass Screen Protector For iPhone 6 / iPhone 6S / iPhone 7 / iPhone 8 (Pack Of 2) Col
# >> 0.68: Black Beauty (Junior Classics)

from typing import List, Tuple
from scipy.stats import gmean

class TwoStageSearch:
    """
    Two-stage search engine that combines TF-IDF and BM25 ranking functions.

    The first stage uses TF-IDF to narrow down the list of potential documents.
    The second stage uses BM25 to re-rank the documents from the first stage
    for more accurate results.

    Attributes:
        documents (List[str]): List of documents to be searched.
        tfidf_engine (TFIDF): Instance of the TFIDF engine.
        bm25_engine (BM25): Instance of the BM25 engine.
    """
    def __init__(self, tfidf_engine, bm25_engine):
        """Initialize the TwoStageSearch instance with the given engines."""
        self.documents = None
        self.tfidf_engine = tfidf_engine
        self.bm25_engine = bm25_engine

    def fit(self, documents: List[str]):
        """Fit the TwoStageSearch model on the given documents."""
        self.documents = documents
        self.tfidf_engine.fit(self.documents)

    def search(
        self,
        query: str,
        limit_stage1: int = 100,
        limit_stage2: int = 5,
        show_results: bool = False,
        char_limit: int = 100
    ) -> List[Tuple[int, float, float, float]]:
        """
        Search for documents similar to a query using a two-stage search process.

        Args:
            query (str): Query string to search for.
            limit_stage1 (int): Number of top results to consider from the first stage. Default is 100.
            limit_stage2 (int): Number of top results to return after the second stage. Default is 5.
            show_results (bool): Flag to indicate whether to display the search results. Default is False.
            char_limit (int): Character limit for displaying document snippets. Default is 100.

        Returns:
            List[Tuple[int, float, float, float]]: List of tuples with document indices and their scores
                from both stages and the aggregated score.
        """
        # Stage 1: TF-IDF search to get candidate documents
        tfidf_results = self.tfidf_engine.search(query, limit=limit_stage1, show_results=False)
        if not tfidf_results:
            return []

        # Get the candidate documents for stage 2
        candidate_indices = [idx for idx, _ in tfidf_results]
        candidate_documents = [self.documents[idx] for idx in candidate_indices]

        # Fit BM25 on the candidate documents
        self.bm25_engine.fit(candidate_documents)

        # Stage 2: BM25 search on candidate documents
        bm25_results = self.bm25_engine.search(query, limit=limit_stage2, show_results=False)

        # Combine the results
        combined_results = []
        tfidf_scores_dict = {idx: score for idx, score in tfidf_results}

        for doc_idx, bm25_score in bm25_results:
            original_idx = candidate_indices[doc_idx]
            tfidf_score = tfidf_scores_dict[original_idx]
            combined_score = gmean([tfidf_score, bm25_score]) if bm25_score > 0 else 0
            combined_results.append((original_idx, tfidf_score, bm25_score, combined_score))

        # Sort by combined score (descending), then by BM25 score, then by TF-IDF score
        combined_results.sort(key=lambda x: (-x[3], -x[2], -x[1]))

        if show_results:
            print(f"Top {limit_stage2} search results for `{query}`:")
            for idx, (doc_idx, tfidf_score, bm25_score, combined_score) in enumerate(combined_results[:limit_stage2], 1):
                preview = self.documents[doc_idx][:char_limit].replace('\n', ' ')
                print(f"{combined_score:.2f}|{bm25_score:.2f}|{tfidf_score:.2f}: {preview}")

        return combined_results[:limit_stage2]

tfidf_engine = TFIDF(show_progress=True)
bm25_engine = BM25(show_progress=True)

ts = TwoStageSearch(tfidf_engine, bm25_engine)
ts.fit('/content/documents.json')
ts.search('iphone charger', show_results=True)
ts.search('iph', show_results=True)
ts.search('eyephone', show_results=True)

# >> Output:
# >> Top 5 search results for `iphone charger`:
# >> 0.64|2.75|1.33: Belkin Boost Up 7.5 W Wireless Charging Pad for iPhone X,8,& 8 Plus with AC Power Adapter (White) Si
# >> 0.64|2.57|1.28: Ozoy Wireless Charger,QI Wireless Charging Pad for Apple iPhone 8/8 Plus, iPhone X, Samsung Note 8,
# >> 0.54|2.64|1.20: Belkin Boost Up Wireless Charging Pad 7.5W - Wireless Charger Optimized for iPhone, Compatible with
# >> 0.37|2.30|0.93: Baseus B® Smart 2 in 1 Wireless Charger for Apple IWatch and Qi Enabled Phone Charger for Apple iPho
# >> 0.32|2.27|0.85: AT&T WC50 5W, Qi-Certified Wireless Charger (Black) Size:WC50   Charge your electronic devices with

# >> Top 5 search results for `iph`:
# >> 0.31|0.00|0.00: Apple iPhone 7 (Jet Black, 32GB)
# >> 0.27|0.00|0.00: POPIO Tempered Glass Screen Protector For iPhone 6 / iPhone 6S / iPhone 7 / iPhone 8 (Pack Of 2) Col
# >> 0.22|0.00|0.00: Apple iPhone 6 (Gold, 1GB RAM, 32GB Storage) Colour:Gold   Apple iPhone 6 (Gold, 32GB)
# >> 0.20|0.00|0.00: Apple iPhone 6S (Rose Gold, 2GB RAM, 32GB Storage)
# >> 0.19|0.00|0.00: Apple iPhone 8 (Space Grey, 2GB RAM, 64GB Storage)

# >> Top 5 search results for `eyephone`:
# >> 0.47|0.00|0.00: Apple iPhone 7 (Jet Black, 32GB)
# >> 0.44|0.00|0.00: Red Eye pack of 3 plain bow tie Red Eye pack of 3 plain bow tie
# >> 0.41|0.00|0.00: Panasonic 2 Handset Telephone Two handset telephone. Portable material. Easy to access. Handle with
# >> 0.40|0.00|0.00: POPIO Tempered Glass Screen Protector For iPhone 6 / iPhone 6S / iPhone 7 / iPhone 8 (Pack Of 2) Col
# >> 0.39|0.00|0.00: Judaism (Eyewitness)